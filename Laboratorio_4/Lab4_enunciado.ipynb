{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ac9155b9f5e04400957a6f8bb3f6610c",
        "deepnote_cell_type": "markdown",
        "id": "2v2D1coL7I8i"
      },
      "source": [
        "<h1><center>Laboratorio 4: La solicitud de Sergio ü§ó</center></h1>\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos - Oto√±o 2025</strong></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d3d6f6d405c54dbe985a5f4b3e4f9120",
        "deepnote_cell_type": "markdown",
        "id": "YxdTmIPD7L_x"
      },
      "source": [
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesores: Stefano Schiappacasse, Sebasti√°n Tinoco\n",
        "- Auxiliares: Melanie Pe√±a, Valentina Rojas\n",
        "- Ayudantes: Angelo Mu√±oz, Valentina Z√∫√±iga"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "851a7788e8214942863cbd4099064ab2",
        "deepnote_cell_type": "markdown",
        "id": "Y2Gyrj-x7N2L"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados\n",
        "\n",
        "- Nombre de alumno 1: Ignacio Negrete\n",
        "- Nombre de alumno 2: Camila Salas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "f23a189afdec4e198683308db70e43b7",
        "deepnote_cell_type": "markdown",
        "id": "jQ9skYc57Pxi"
      },
      "source": [
        "### **Link de repositorio de GitHub:** [Insertar Repositorio](https://github.com/ignacio3645/MDS7202.git)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b5318f41cda64d4290a7a548956ed725",
        "deepnote_cell_type": "markdown",
        "id": "1M4PoEWm7S80"
      },
      "source": [
        "## Temas a tratar\n",
        "- Aplicar Pandas para obtener caracter√≠sticas de un DataFrame.\n",
        "- Aplicar Pipelines y Column Transformers.\n",
        "- Utilizar diferentes algoritmos de cluster y ver el desempe√±o.\n",
        "\n",
        "## Reglas:\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Fecha de entrega: 6 d√≠as de plazo con descuento de 1 punto por d√≠a. Entregas Martes a las 23:59.\n",
        "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda fuertemente asistir.\n",
        "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser√° debidamente penalizado con el reglamento de la escuela.\n",
        "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est√©n en u-cursos no ser√°n revisados. Recuerden que el repositorio tambi√©n tiene nota.\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
        "- Pueden usar cualquier material del curso que estimen conveniente.\n",
        "\n",
        "### Objetivos principales del laboratorio\n",
        "- Comprender c√≥mo aplicar pipelines de Scikit-Learn para generar clusters.\n",
        "- Familiarizarse con plotly.\n",
        "\n",
        "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `numpy`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre arreglos (*o tensores*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "858df483d9e64780a21674afed1d34b8",
        "deepnote_cell_type": "markdown",
        "id": "SuMbiyQZG2Cc"
      },
      "source": [
        "## Descripci√≥n del laboratorio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "403ffe48ec994afda4b91e670a08d0ef",
        "deepnote_cell_type": "markdown",
        "id": "QZsNO4rUrqCz"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/5a/a6/af/5aa6afde8490da403a21601adf7a7240.gif\" width=400 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "0303baa17d4546feae8c9b88c58470bf",
        "deepnote_cell_type": "markdown",
        "id": "2o0MPuk8rqCz"
      },
      "source": [
        "En el coraz√≥n de las operaciones de Aerol√≠nea Lucero, Sergio, el gerente de an√°lisis de datos, reuni√≥ a un talentoso equipo de j√≥venes cient√≠ficos de datos para un desaf√≠o crucial: segmentar la base de datos de los clientes. ‚ÄúNuestro objetivo es descubrir patrones en el comportamiento de los pasajeros que nos permitan personalizar servicios y optimizar nuestras campa√±as de marketing,‚Äù explic√≥ Sergio, mientras desplegaba un amplio rango de datos que inclu√≠an desde h√°bitos de compra hasta opiniones sobre los vuelos.\n",
        "\n",
        "Sergio encarg√≥ a los cient√≠ficos de datos la tarea de aplicar t√©cnicas avanzadas de clustering para identificar distintos segmentos de clientes, como los viajeros frecuentes y aquellos que eligen la aerol√≠nea para celebrar ocasiones especiales. La meta principal era entender profundamente c√≥mo estos grupos perciben la calidad y satisfacci√≥n de los servicios ofrecidos por la aerol√≠nea.\n",
        "\n",
        "A trav√©s de un enfoque meticuloso y colaborativo, los cient√≠ficos de datos se abocaron a la tarea, buscando transformar los datos brutos en valiosos insights que permitir√≠an a Aerol√≠nea Lucero no solo mejorar su servicio, sino tambi√©n fortalecer las relaciones con sus clientes mediante una oferta m√°s personalizada y efectiva."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e78cb41b144041af98928ab26dcfdaa9",
        "deepnote_cell_type": "markdown",
        "id": "hs4KKWF1Hdpo"
      },
      "source": [
        "## Importamos librerias utiles üò∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "cell_id": "95a5533cfd6d49cfb9afc111c44d224f",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 15,
        "execution_start": 1714107106552,
        "id": "a4YpMafirqC0",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "acbeab32db6146678e75448dddf43da8",
        "deepnote_cell_type": "markdown",
        "id": "UQOXod4gHhSq"
      },
      "source": [
        "## 1. Estudio de Performance üìà [10 Puntos]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "704b56b978254ad3ae12cdbf58f4832d",
        "deepnote_cell_type": "markdown",
        "id": "Gn5u5ICkrqC2"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/23/b7/6e/23b76e9e77e63c0eec1a7b28372369e3.gif\" width=300>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d35fbdcc5ef045d6a2822622f0714179",
        "deepnote_cell_type": "markdown",
        "id": "y4Z0jTjtrqC2"
      },
      "source": [
        "Don Sergio les ha encomendado su primera tarea: analizar diversas t√©cnicas de clustering. Su objetivo es entender detalladamente c√≥mo funcionan estos m√©todos en t√©rminos de segmentaci√≥n y eficiencia en tiempo de ejecuci√≥n.\n",
        "\n",
        "Analice y compare el desempe√±o, tiempo de ejecuci√≥n y visualizaciones de cuatro algoritmos de clustering (k-means, DBSCAN, Ward y GMM) aplicados a tres conjuntos de datos, incrementando progresivamente su tama√±o. Utilice Plotly para las gr√°ficas y discuta los resultados tanto cualitativa como cuantitativamente.\n",
        "\n",
        "Uno de los requisitos establecidos por Sergio es que el an√°lisis se lleve a cabo utilizando Plotly; de no ser as√≠, se considerar√° incorrecto. Para facilitar este proceso, se ha proporcionado un c√≥digo de Plotly que puede servir como base para realizar las gr√°ficas. Ap√≥yese en el c√≥digo entregado para efectuar el an√°lisis y tome como referencia la siguiente imagen para realizar los gr√°ficos:\n",
        "\n",
        "<img src='https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/misc_images/Screenshot_2024-04-26_at_9.10.44_AM.png' width=800 />\n",
        "\n",
        "En el gr√°fico se visualizan en dos dimensiones los diferentes tipos de datos proporcionados en `datasets`. Cada columna corresponde a un modelo de clustering diferente, mientras que cada fila representa un conjunto de datos distinto. Cada uno de los gr√°ficos incluye el tiempo en segundos que tarda el an√°lisis y la m√©trica Silhouette obtenida."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "37580aab6cef4238a8ce42c50a6d35de",
        "deepnote_cell_type": "markdown",
        "id": "maCUNAvZrqC2"
      },
      "source": [
        "Para ser m√°s espec√≠ficos, usted debe cumplir los siguientes objetivos:\n",
        "1. Generar una funci√≥n que permita replicar el gr√°fico expuesto en la imagen (no importa que los colores calcen). [4 puntos]\n",
        "2. Ejecuta la funci√≥n para un `n_samples` igual a 1000, 5000, 10000. [2 puntos]\n",
        "3. Analice y compare el desempe√±o, tiempo de ejecuci√≥n y visualizaciones de cuatro algoritmos de clustering utilizando las 3 configuraciones dadas en `n_samples`. [4 puntos]\n",
        "\n",
        "\n",
        "> ‚ùó Tiene libertad absoluta de escoger los hiper par√°metros de los cluster, sin embargo, se recomienda verificar el dominio de las variables para realizar la segmentaci√≥n.\n",
        "\n",
        "> ‚ùó Recuerde que es obligatorio el uso de plotly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "cell_id": "7f7c25e366754595b13fc2e8116f65a0",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 78,
        "execution_start": 1714107108441,
        "id": "i0IZPGPOrqC3",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "En la siguiente celda se crean los datos ficticios a usar en la secci√≥n 1 del lab.\n",
        "‚ùóNo realice cambios a esta celda a excepci√≥n de n_samples‚ùó\n",
        "\"\"\"\n",
        "\n",
        "# Datos a utilizar\n",
        "\n",
        "# Configuracion\n",
        "n_samples = 5000 #Este par√°metro si lo pueden modificar\n",
        "\n",
        "def create_data(n_samples):\n",
        "\n",
        "    # Lunas\n",
        "    moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=30)\n",
        "    # Blobs\n",
        "    blobs = datasets.make_blobs(n_samples=n_samples, random_state=172)\n",
        "    # Datos desiguales\n",
        "    transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
        "    mutated = (np.dot(blobs[0], transformation), blobs[1])\n",
        "\n",
        "    # Generamos Dataset\n",
        "    dataset = {\n",
        "        'moons':{\n",
        "            'x': moons[0], 'classes': moons[1], 'n_cluster': 2\n",
        "        },\n",
        "        'blobs':{\n",
        "            'x': blobs[0], 'classes': blobs[1], 'n_cluster': 3\n",
        "        },\n",
        "        'mutated':{\n",
        "            'x': mutated[0], 'classes': mutated[1], 'n_cluster': 3\n",
        "        }\n",
        "    }\n",
        "    return dataset\n",
        "\n",
        "data_sets = create_data(n_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y51s6f_UtIkc"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "import time\n",
        "\n",
        "def crear_scatter_plot(X, labels, tiempo, silhouette):\n",
        "    return go.Scatter(\n",
        "        x=X[:, 0],\n",
        "        y=X[:, 1],\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            color=labels,\n",
        "            colorscale='Viridis',\n",
        "            size=5,\n",
        "            line=dict(width=0),\n",
        "            showscale=False\n",
        "        ),\n",
        "        showlegend=False,\n",
        "        text=[f\"Tiempo: {tiempo:.2f}s<br>Silhouette: {silhouette:.2f}\"] * len(labels),\n",
        "        hoverinfo='text'\n",
        "    )\n",
        "\n",
        "def graficar_grid(dataset_dict, clustering_model_templates, model_names):\n",
        "    n_rows = len(dataset_dict)\n",
        "    n_cols = len(clustering_model_templates)\n",
        "\n",
        "    fig = make_subplots(\n",
        "        rows=n_rows, cols=n_cols,\n",
        "        subplot_titles=[\n",
        "            f\"{model_names[col]}\" if row == 0 else \"\" \n",
        "            for row in range(n_rows) for col in range(n_cols)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "    for i, (dataset_name, datos) in enumerate(dataset_dict.items()):\n",
        "        X = datos['x']\n",
        "        n_clusters = datos['n_cluster']\n",
        "\n",
        "        for j, model_template in enumerate(clustering_model_templates):\n",
        "            # Clonamos el modelo (para evitar conflictos de estado interno entre datasets)\n",
        "            model = model_template(n_clusters) if callable(model_template) else model_template\n",
        "\n",
        "            start = time.time()\n",
        "            if hasattr(model, \"fit_predict\"):\n",
        "                labels = model.fit_predict(X)\n",
        "            else:\n",
        "                model.fit(X)\n",
        "                labels = model.predict(X)\n",
        "            end = time.time()\n",
        "            tiempo = end - start\n",
        "\n",
        "            sil_score = silhouette_score(X, labels) if len(set(labels)) > 1 else 0\n",
        "\n",
        "            scatter = crear_scatter_plot(X, labels, tiempo, sil_score)\n",
        "            fig.add_trace(scatter, row=i+1, col=j+1)\n",
        "\n",
        "            fig.add_annotation(\n",
        "                text=f\"{tiempo:.2f} [s] | s: {sil_score:.2f}\",\n",
        "                showarrow=False,\n",
        "                xref=\"x domain\", yref=\"y domain\",\n",
        "                x=0.5, y=1.1,\n",
        "                row=i+1, col=j+1,\n",
        "                font=dict(size=10)\n",
        "            )\n",
        "\n",
        "    fig.update_layout(height=700, width=1200, title_text=\"Comparaci√≥n de t√©cnicas de clustering\")\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "clustering_models = [\n",
        "    lambda k: KMeans(n_clusters=k),\n",
        "    lambda k: GaussianMixture(n_components=k),\n",
        "    lambda k: AgglomerativeClustering(n_clusters=k, linkage='ward'),\n",
        "    lambda k: DBSCAN(eps=0.2)\n",
        "]\n",
        "\n",
        "model_names = [\"KMeans\", \"GMM\", \"WARD\", \"DBSCAN\"]\n",
        "\n",
        "data_1000 = create_data(1000)\n",
        "\n",
        "graficar_grid(data_1000, clustering_models, model_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clustering_models = [\n",
        "    lambda k: KMeans(n_clusters=k),\n",
        "    lambda k: GaussianMixture(n_components=k),\n",
        "    lambda k: AgglomerativeClustering(n_clusters=k, linkage='ward'),\n",
        "    lambda k: DBSCAN(eps=0.2)\n",
        "]\n",
        "\n",
        "model_names = [\"KMeans\", \"GMM\", \"WARD\", \"DBSCAN\"]\n",
        "\n",
        "data_5000 = create_data(5000)\n",
        "\n",
        "graficar_grid(data_5000, clustering_models, model_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clustering_models = [\n",
        "    lambda k: KMeans(n_clusters=k),\n",
        "    lambda k: GaussianMixture(n_components=k),\n",
        "    lambda k: AgglomerativeClustering(n_clusters=k, linkage='ward'),\n",
        "    lambda k: DBSCAN(eps=0.2)\n",
        "]\n",
        "\n",
        "model_names = [\"KMeans\", \"GMM\", \"WARD\", \"DBSCAN\"]\n",
        "\n",
        "data_10000 = create_data(10000)\n",
        "\n",
        "graficar_grid(data_10000, clustering_models, model_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Respuesta 1.3:** \n",
        "\n",
        "En cuanto al rendimiento de los clusters, se puede apreciar que, para el primer dataset correspondiente a moons, DBSCAN es el que mejor logra capturar los clusters debido a su capacidad de agrupar por densidad. En cuanto al segundo dataset, tanto K-Means como GMM y Ward logran capturar de muy buena forma el comportamiento de los clusters. Finalmente, para el √∫ltimo dataset, solo GMM logr√≥ separar correctamente los clusters.\n",
        "\n",
        "Por otra parte, en cuanto al tiempo de procesamiento para la generaci√≥n de clusters en los distintos modelos, no existe mayor diferencia entre datasets. Luego, la diferencia de tiempo entre modelos no es muy significativa variando la cantidad de datos, a excepci√≥n del modelo aglomerativo Ward. En este, se observa un aumento no lineal en el tiempo que demora con respecto a la cantidad de datos entregados. En efecto, parecer√≠a ser de complejidad logar√≠tmica, pues al haber un aumento de 5 veces en la cantidad de datos, se observa un aumento de 21 veces en el tiempo. Luego, al haber un aumento de 2 veces en la cantidad de datos, el tiempo se cuadruplica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "13c5cb8067d9415f83b3d497954a437a",
        "deepnote_cell_type": "markdown",
        "id": "3mCbZc86rqC6"
      },
      "source": [
        "## 2. An√°lisis de Satisfacci√≥n de Vuelos. [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "fd6e991646b44f50a4b13f01d1542415",
        "deepnote_cell_type": "markdown",
        "id": "JI33m5jbrqC6"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.gifer.com/2Hci.gif\" width=400 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5742dfbd5a2e43778ff250436bab1005",
        "deepnote_cell_type": "markdown",
        "id": "h5k24znirqC7"
      },
      "source": [
        "Habiendo entendido c√≥mo funcionan los modelos de aprendizaje no supervisado, *Don Sergio* le encomienda estudiar la satisfacci√≥n de pasajeros al haber tomado un vuelo en alguna de sus aerolineas. Para esto, el magnate le dispone del dataset `aerolineas_licer.parquet`, el cual contiene el grado de satisfacci√≥n de los clientes frente a diferentes aspectos del vuelo. Las caracter√≠sticas del vuelo se definen a continuaci√≥n:\n",
        "\n",
        "- *Gender*: G√©nero de los pasajeros (Femenino, Masculino)\n",
        "- *Customer Type*: Tipo de cliente (Cliente habitual, cliente no habitual)\n",
        "- *Age*: Edad actual de los pasajeros\n",
        "- *Type of Travel*: Prop√≥sito del vuelo de los pasajeros (Viaje personal, Viaje de negocios)\n",
        "- *Class*: Clase de viaje en el avi√≥n de los pasajeros (Business, Eco, Eco Plus)\n",
        "- *Flight distance*: Distancia del vuelo de este viaje\n",
        "- *Inflight wifi service*: Nivel de satisfacci√≥n del servicio de wifi durante el vuelo (0:No Aplicable; 1-5)\n",
        "- *Departure/Arrival time convenient*: Nivel de satisfacci√≥n con la conveniencia del horario de salida/llegada\n",
        "- *Ease of Online booking*: Nivel de satisfacci√≥n con la facilidad de reserva en l√≠nea\n",
        "- *Gate location*: Nivel de satisfacci√≥n con la ubicaci√≥n de la puerta\n",
        "- *Food and drink*: Nivel de satisfacci√≥n con la comida y la bebida\n",
        "- *Online boarding*: Nivel de satisfacci√≥n con el embarque en l√≠nea\n",
        "- *Seat comfort*: Nivel de satisfacci√≥n con la comodidad del asiento\n",
        "- *Inflight entertainment*: Nivel de satisfacci√≥n con el entretenimiento durante el vuelo\n",
        "- *On-board service*: Nivel de satisfacci√≥n con el servicio a bordo\n",
        "- *Leg room service*: Nivel de satisfacci√≥n con el espacio para las piernas\n",
        "- *Baggage handling*: Nivel de satisfacci√≥n con el manejo del equipaje\n",
        "- *Check-in service*: Nivel de satisfacci√≥n con el servicio de check-in\n",
        "- *Inflight service*: Nivel de satisfacci√≥n con el servicio durante el vuelo\n",
        "- *Cleanliness*: Nivel de satisfacci√≥n con la limpieza\n",
        "- *Departure Delay in Minutes*: Minutos de retraso en la salida\n",
        "- *Arrival Delay in Minutes*: Minutos de retraso en la llegada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOoIFHpw5xCW"
      },
      "source": [
        "En consideraci√≥n de lo anterior, realice las siguientes tareas:\n",
        "\n",
        "0. Ingeste el dataset a su ambiente de trabajo.\n",
        "\n",
        "1. Seleccione **s√≥lo las variables num√©ricas del dataset**.  Explique qu√© √©fectos podr√≠a causar el uso de variables categ√≥ricas en un algoritmo no supervisado. [2 punto]\n",
        "\n",
        "2. Realice una visualizaci√≥n de la distribuci√≥n de cada variable y analice cada una de estas distribuciones. [2 punto]\n",
        "\n",
        "3. Bas√°ndose en los gr√°ficos, eval√∫e la necesidad de escalar los datos y explique el motivo de su decisi√≥n. [2 puntos]\n",
        "\n",
        "4. Examine la correlaci√≥n entre las variables mediante un correlograma. [2 puntos]\n",
        "\n",
        "5. De acuerdo con los resultados obtenidos en 5, reduzca la dimensionalidad del conjunto de datos a cuatro variables, justificando su elecci√≥n respecto a las variables que decide eliminar. [2 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO6tcVBCtxxS"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzHTZ17xveU_"
      },
      "outputs": [],
      "source": [
        "# Carga de datos\n",
        "df_vuelos= pd.read_parquet('aerolineas_lucer.parquet')\n",
        "df_vuelos.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se seleccionan solo variables numericas de df\n",
        "df_vuelos_n = df_vuelos.drop(columns=['Gender', 'Customer Type', 'Type of Travel', 'Class'])\n",
        "df_vuelos_n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Respuesta 2.1:** Los efectos que podr√≠an causar una variable categ√≥rica en un algoritmo de aprendizaje no supervisado podr√≠an ser:\n",
        "\n",
        "* No detectar de buena forma los patrones en los datos si no se pasan a variables n√∫mericas\n",
        "* No poder aplicarse debido a que se utilizan ecuaciones en algunos algoritmos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se observa la distribuci√≥n de las distintas variables\n",
        "columns = df_vuelos_n.drop(columns='id').columns\n",
        "fig = make_subplots(rows=6, cols=3, subplot_titles=columns)\n",
        "\n",
        "# Se agregan las distribuciones\n",
        "for i in range(len(columns)):\n",
        "    fig.add_trace(go.Histogram(x=df_vuelos_n[columns[i]]), row=i//3 + 1, col=i%3 + 1)\n",
        "\n",
        "fig.update_layout(height=1000, width=1000, title_text=\"Distribuci√≥n de variables num√©ricas\")\n",
        "    \n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Respuesta 2.2:** \n",
        "\n",
        "Debido a que se presentan comportamiento similares entre gr√°ficos, se repetir√° el an√°lisis, teniendo en consideraci√≥n que pueden variar levemente entre ellos, pero considerando este an√°lisis como su comportamiento general.\n",
        "\n",
        "* Age: se observa que va variando la cantidad de datos disponibles para cada dato correspondiente en x, sin tener un comportamiento constante.\n",
        "* Flight distance: se presenta un mayor rango de datos, pero con una tendencia a disminuir a medida que se avanza en el eje x.\n",
        "* Inflight wifi service,  Ease of Online booking: los tipos de datos distintos disminuye, pero con un aumento considerable en la cantidad de cada dato. Adem√°s, se observa un aumento y luego disminuci√≥n de la cantidad de estos.\n",
        "* Departure, Inflight entertainment, Leg room service, Online boarding, Baggage handling, Food and drink: tambi√©n se presenta una menor cantidad de datos distintos, pero con un aumento de ellos a medida que se avanza en el eje x, para luego disminuir en el √∫ltimo dato.\n",
        "* Gate Location: se observa un aumento y luego disminuci√≥n en la cantidad de datos, con la mayor cantidad en el valor 3 en x y sin valores para 0. \n",
        "* Seat comfort, On-board service, Inflight service, Cleanliness: en este caso es similar al cuarto punto, aumentando y luego disminuyendo solo en el √∫ltimo valor, pero sin el dato 0.\n",
        "* Checkin service: en este caso se puede ver que no se presentan valores para 0, que se mantiene un comportamiento similar en cantidad para 1 y 2, y luego para 3 y 4, para luego volver a disminuir en el dato 5. \n",
        "* Departure Delay in Minutes, Arrival Delay in Minutes: se puede notar que los datos se encuentran mayoritariamente en valores bajos y en gran cantidad, por lo que cuesta observar de mejor manera su comportamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Respuesta 2.3:**  \n",
        "\n",
        "Se puede observar que los rangos en los que se mueven y la manera en la que se distribuyen los datos son diferentes, por esto es que se decide escalarlos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se escalan los datos\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df_vuelos_escalados = scaler.fit_transform(df_vuelos_n.drop(columns='id'))\n",
        "df_vuelos_escalados = pd.DataFrame(df_vuelos_escalados, columns=df_vuelos_n.drop(columns='id').columns)\n",
        "df_vuelos_escalados.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_vuelos_escalados.mean(), df_vuelos_escalados.std()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se observa la distribuci√≥n de las distintas variables\n",
        "columns = df_vuelos_escalados.columns\n",
        "fig2 = make_subplots(rows=6, cols=3, subplot_titles=columns)\n",
        "\n",
        "# Se agregan las distribuciones\n",
        "for i in range(len(columns)):\n",
        "    fig2.add_trace(go.Histogram(x=df_vuelos_escalados[columns[i]]), row=i//3 + 1, col=i%3 + 1)\n",
        "\n",
        "fig2.update_layout(height=1000, width=1000, title_text=\"Distribuci√≥n de variables num√©ricas con datos escalados\")\n",
        "    \n",
        "fig2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora se observa que se mueven a lo largo del mismo rango en x, pero notando que siguen presentando comportamientos similares a los datos no escalados. Adem√°s, en los √∫ltimos 2 gr√°ficos, se sigue dificultando el observar los datos debido a sus valores bastante bajos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se genera un correlograma \n",
        "df_corr = df_vuelos_escalados.corr()\n",
        "df_corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se grafica \n",
        "fig_c = px.imshow(df_corr, \n",
        "                  title='Correlaci√≥n entre variables num√©ricas', text_auto='.2f')\n",
        "fig_c.update_layout(width=800, height=800)\n",
        "\n",
        "fig_c.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se suman los valores de las variables para poder saber las que presentan una mayor correlacion\n",
        "corr = df_vuelos_escalados.corr().abs().sum()\n",
        "corr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Respuesta 2.5:**\n",
        "\n",
        "Al observar el valor resultante de la suma de las correlaciones para cada variable, se consideran las columnas que presentan un menor valor, ya que son precisamente las que aunque tengan correlaci√≥n con algunas variables, esta es la menor posible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se eligen las variables que no presentan mayor correlacion con las otras\n",
        "df_vuelos_escalados = df_vuelos_escalados[['Age', 'Flight Distance', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']]\n",
        "df_vuelos_escalados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "4b6c047d994f40ea9e78a36a777042e0",
        "deepnote_cell_type": "markdown",
        "id": "PNGfTgtkrqC9"
      },
      "source": [
        "## 3. Preprocesamiento üé≠. [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "713b3f0e61dd4841bb5b38c730d344d5",
        "deepnote_cell_type": "markdown",
        "id": "6RZD0fMNrqC-"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://media4.giphy.com/media/vWst8QUOKAot6MHEZe/giphy.gif?cid=6c09b952gm5xylrj4k5caq2slgwivx9azbgb0ox297sk5zjx&ep=v1_internal_gif_by_id&rid=giphy.gif&ct=g\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "98400c7b5fec4af193eec3601f53891e",
        "deepnote_cell_type": "markdown",
        "id": "J6d4VEOTrqC-"
      },
      "source": [
        "Tras quedar satisfecho con los resultados presentados en el punto 2, el due√±o de la empresa ha solicitado que se preprocesen los datos mediante un `pipeline`. Es crucial que este proceso tenga en cuenta las observaciones derivadas de los an√°lisis anteriores. Adicionalmente, ha expresado su inter√©s en visualizar el conjunto de datos en un gr√°fico de dos o tres dimensiones.\n",
        "\n",
        "Bas√°ndose en los an√°lisis realizados anteriormente:\n",
        "1. Cree un `pipeline` que incluya PCA, utilizando las consideraciones mencionadas previamente para proyectar los datos a dos dimensiones. [4 puntos]\n",
        "2. Grafique los resultados obtenidos y comente lo visualizado. [6 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paDSaGoq0OUp"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "ad1e70818ad748638ca0927b07a76125",
        "deepnote_cell_type": "code",
        "id": "gBYG238wrqC-"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('pca', PCA(n_components=2))\n",
        "])\n",
        "\n",
        "# Aplicamos el pipeline\n",
        "X_pca = pipeline.fit_transform(df_vuelos_escalados)\n",
        "\n",
        "# DataFrame con los componentes principales\n",
        "df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
        "fig = px.scatter(\n",
        "    df_pca, \n",
        "    x='PC1', \n",
        "    y='PC2', \n",
        "    title='Distribuci√≥n en componentes principales (PCA)',\n",
        "    labels={'PC1': 'Componente Principal 1', 'PC2': 'Componente Principal 2'}\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Respuestas 3.2:** El gr√°fico de componentes principales (PCA) muestra una distribuci√≥n densa y alargada de los datos a lo largo de la primera componente, lo que sugiere que gran parte de la varianza del conjunto se concentra en esa direcci√≥n. La forma afilada y la baja dispersi√≥n en la segunda componente indican una alta correlaci√≥n entre las variables originales y una posible redundancia en la informaci√≥n que es esperable dada la naturaleza de nuestros datos . Adem√°s, no se observan agrupaciones evidentes en el espacio  lo que sugiere que los datos no presentan una separaci√≥n clara en clusters, al menos en este plano (2 componentes)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "bd281470d3054764a63d857cfa7d52a6",
        "deepnote_cell_type": "text-cell-h2",
        "formattedRanges": [],
        "id": "7ENoOtIIrqC_"
      },
      "source": [
        "## 4. Outliers üö´üôÖ‚Äç‚ôÄÔ∏è‚ùåüôÖ‚Äç‚ôÇÔ∏è [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "db89e9c9f35c44abbd8991180226c0ea",
        "deepnote_cell_type": "markdown",
        "id": "fbGw6Sa-rqC_"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://joachim-gassen.github.io/images/ani_sim_bad_leverage.gif\" width=250>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3e2f59fa12954641af7a854a4e203694",
        "deepnote_cell_type": "markdown",
        "id": "nl_ccu9brqDA"
      },
      "source": [
        "Con el objetivo de mantener la claridad en su an√°lisis, Don Sergio le ha solicitado entrenar un modelo que identifique pasajeros con comportamientos altamente at√≠picos.\n",
        "\n",
        "1. Utilice `IsolationForest` para clasificar las anomal√≠as del dataset (sin aplicar PCA), configurando el modelo para que s√≥lo el 1% de los datos sean considerados an√≥malos. Aseg√∫rese de integrar esta tarea dentro de un `pipeline`. [3 puntos]\n",
        "\n",
        "2. Visualice los resultados en el gr√°fico de dos dimensiones previamente creado. [3 puntos]\n",
        "\n",
        "3. ¬øC√≥mo evaluar√≠a el rendimiento de su modelo en la detecci√≥n de anomal√≠as? [4 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5cS1FR00NlF"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "be86896911244aa89e3b5f3f00a286af",
        "deepnote_cell_type": "code",
        "id": "iaPZFmjyrqDA"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "pipeline_iforest = Pipeline([\n",
        "    ('iforest', IsolationForest(contamination=0.01, random_state=42))\n",
        "])\n",
        "\n",
        "predicciones = pipeline_iforest.fit_predict(df_vuelos_escalados)\n",
        "\n",
        "df_vuelos_escalados['anomaly'] = predicciones\n",
        "\n",
        "df_vuelos_escalados = df_vuelos_escalados.reset_index(drop=True)\n",
        "df_pca = df_pca.reset_index(drop=True)\n",
        "\n",
        "df_pca['anomaly'] = df_vuelos_escalados['anomaly']\n",
        "\n",
        "\n",
        "fig = px.scatter(\n",
        "    df_pca,\n",
        "    x='PC1',\n",
        "    y='PC2',\n",
        "    color=df_pca['anomaly'].map({1: 'Normal', -1: 'Anomal√≠a'}),\n",
        "    title='Detecci√≥n de Anomal√≠as con IsolationForest',\n",
        "    labels={'PC1': 'Componente Principal 1', 'PC2': 'Componente Principal 2'}\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Respuesta 4.3:**  Si bien la distribuci√≥n de los datos se encuentra repartida en todo el plano, es claro ver que hay una mayor concentraci√≥n en la parte izquierda, lo que indica que muchas observaciones comparten caracter√≠sticas similares en ese sector del espacio. Esto resulta contraintuitivo si estamos considerando esa zona como puntos en los que hay outliers. Es por esto que el rendimiento del modelo en la detecci√≥n de anomal√≠as se podr√≠a considerar como deficiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3871e2fe5bdd422dbdbfaebf75503ae3",
        "deepnote_cell_type": "markdown",
        "id": "zQFTklmVrqDB"
      },
      "source": [
        "## 5. M√©tricas de Desempe√±o üöÄ [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "236333de6dd445c182aefcc507589325",
        "deepnote_cell_type": "markdown",
        "id": "YpNj4wbPrqDB"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://giffiles.alphacoders.com/219/219081.gif\" width=300>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a7e1ceb91be94b1da2ab8be97dfac999",
        "deepnote_cell_type": "markdown",
        "id": "CR3hzRxrrqDB"
      },
      "source": [
        "Motivado por incrementar su fortuna, Don Sergio le solicita entrenar un modelo que le permita segmentar a los pasajeros en grupos distintos, con el objetivo de optimizar las diversas campa√±as de marketing dise√±adas por su equipo. Para ello, le se pide realizar las siguientes tareas:\n",
        "\n",
        "1. Utilizar el modelo **Gaussian Mixture** y explore diferentes configuraciones de n√∫mero de cl√∫sters, espec√≠ficamente entre 3 y 8. Aseg√∫rese de integrar esta operaci√≥n dentro de un `pipeline`. [4 puntos]\n",
        "2. Explique cu√°l ser√≠a el criterio adecuado para seleccionar el n√∫mero √≥ptimo de cl√∫sters. **Justifique de forma estadistica y a traves de gr√°ficos.** [6 puntos]\n",
        "\n",
        "> **HINT:** Se recomienda investigar sobre los criterios AIC y BIC para esta tarea."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt_T_zTg0MXB"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se vuelve a determinar df_pca como lo que se tenia en 3\n",
        "df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "6d3d1bb3fda14321984466d9101a775a",
        "deepnote_cell_type": "code",
        "id": "5GeUb9J3rqDB"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "# Para 3 componentes\n",
        "# Se crea el pipeline con Gaussian Mixture\n",
        "pipeline_desempenno = Pipeline([('gaussian', GaussianMixture(n_components=3))])\n",
        "\n",
        "pipeline_desempenno.fit(X_pca)\n",
        "\n",
        "labels = pipeline_desempenno.predict(X_pca)\n",
        "\n",
        "# Se grafica\n",
        "fig = px.scatter(\n",
        "    df_pca,\n",
        "    x='PC1',\n",
        "    y='PC2',\n",
        "    color=labels,\n",
        "    title='Clusters utilizando Gaussian Mixture Model'\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Para 5 componentes\n",
        "# Se crea el pipeline con Gaussian Mixture\n",
        "pipeline_desempenno = Pipeline([('gaussian', GaussianMixture(n_components=5))])\n",
        "\n",
        "pipeline_desempenno.fit(X_pca)\n",
        "\n",
        "labels = pipeline_desempenno.predict(X_pca)\n",
        "\n",
        "# Se grafica\n",
        "fig = px.scatter(\n",
        "    df_pca,\n",
        "    x='PC1',\n",
        "    y='PC2',\n",
        "    color=labels,\n",
        "    title='Clusters utilizando Gaussian Mixture Model'\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Para 7 componentes\n",
        "# Se crea el pipeline con Gaussian Mixture\n",
        "pipeline_desempenno = Pipeline([('gaussian', GaussianMixture(n_components=7))])\n",
        "\n",
        "pipeline_desempenno.fit(X_pca)\n",
        "\n",
        "labels = pipeline_desempenno.predict(X_pca)\n",
        "\n",
        "# Se grafica\n",
        "fig = px.scatter(\n",
        "    df_pca,\n",
        "    x='PC1',\n",
        "    y='PC2',\n",
        "    color=labels,\n",
        "    title='Clusters utilizando Gaussian Mixture Model'\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Se busca el numero optimo de clusters\n",
        "def bic_score(estimador, X):\n",
        "    return -estimador.bic(X)\n",
        "\n",
        "def aic_score(estimador, X):\n",
        "    return -estimador.aic(X)\n",
        "\n",
        "parametros = {'n_components': [3,4,5,6,7,8]} # componentes a evaluar\n",
        "\n",
        "# Se utiliza GridSearchCV para obtener los valores de AIC y BIC\n",
        "results_bic = GridSearchCV(GaussianMixture(), parametros, scoring=bic_score)\n",
        "\n",
        "results_aic = GridSearchCV(GaussianMixture(), parametros, scoring=aic_score)\n",
        "\n",
        "results_bic.fit(X_pca)\n",
        "\n",
        "results_aic.fit(X_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se obtienen los resultados para bic\n",
        "resultsbicdf = pd.DataFrame(results_bic.cv_results_)[['param_n_components', 'mean_test_score', 'std_test_score']]\n",
        "resultsbicdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se obtienen los resultados para aic\n",
        "resultsaicdf = pd.DataFrame(results_aic.cv_results_)[['param_n_components', 'mean_test_score', 'std_test_score']]\n",
        "resultsaicdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se grafican\n",
        "\n",
        "fig1 = px.line(resultsbicdf, x='param_n_components', y='mean_test_score', title='Valores de BIC', height=400, width=700)\n",
        "fig2 = px.line(resultsaicdf, x='param_n_components', y='mean_test_score', title='Valores de AIC', height=400, width=700)\n",
        "\n",
        "fig1.show()\n",
        "fig2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Respuesta 5.2:** el criterio adecuado para seleccionar la cantidad de clusters es considerar los menores valores para BIC y AIC (en mean_test_score), los cuales castigan la complejidad que se pueda presentar en el modelo, uno de manera m√°s fuerte que el otro. Adem√°s, se tiene la desviaci√≥n est√°ndar, donde un menor valor tambi√©n representa un mejor modelo debido a que es m√°s estable.\n",
        "\n",
        "Se puede notar que en BIC y AIC se presenta un menor valor al utilizar 3 componentes (tanto en el dataframe como en el gr√°fico para mean_test_score), pero al observar la desviaci√≥n, se puede ver que en el caso de AIC no se obtiene el menor valor, lo que puede deberse a la forma en la que penaliza AIC en comparaci√≥n con BIC. De todas formas, considerando que no hay una gran diferencia en el valor de desviaci√≥n entre los 3 y 6 componentes para lo obtenido al utilizar AIC, pero en BIC y AIC al elegir 3 componentes se obtiene el menor valor, se considerar√° como √≥ptima esta cantidad de clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "dd342e336254418ba766b29dce16b267",
        "deepnote_cell_type": "markdown",
        "id": "P9CERnaerqDC"
      },
      "source": [
        "## 6. An√°lisis de resultados üìä [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "953b5ad01a704b50b899db7176d1b7b2",
        "deepnote_cell_type": "markdown",
        "id": "I1yNa111rqDC"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://www.icegif.com/wp-content/uploads/2021/12/icegif-1407.gif\" width=300>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "fd90e2f135404353ac0b5ab844936ca7",
        "deepnote_cell_type": "markdown",
        "id": "dg0Qx4RZrqDC"
      },
      "source": [
        "Una vez identificado el n√∫mero √≥ptimo de cl√∫sters, se le pide realizar lo siguiente:\n",
        "\n",
        "1. Utilizar la proyecci√≥n en dos dimensiones para visualizar cada cl√∫ster claramente. [2 puntos]\n",
        "\n",
        "2. ¬øEs posible distinguir claramente entre los cl√∫sters generados? [2 puntos]\n",
        "\n",
        "3. Proporcionar una descripci√≥n breve de cada cl√∫ster utilizando estad√≠sticas descriptivas b√°sicas, como la media y la desviaci√≥n est√°ndar, para resumir las caracter√≠sticas de las variables utilizadas en estos algoritmos. [2 puntos]\n",
        "\n",
        "4. Proceda a visualizar los cl√∫sters en tres dimensiones para una perspectiva m√°s detallada. [2 puntos]\n",
        "\n",
        "5. ¬øC√≥mo afecta esto a sus conclusiones anteriores? [2 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRN0zZip0IMB"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "9abf4dbc643e40cebe99fcb1ff3ff413",
        "deepnote_cell_type": "code",
        "id": "XmZrz15GrqDC"
      },
      "outputs": [],
      "source": [
        "# Se elige como optimo 3 componentes y se visualizan\n",
        "pipeline_desempenno = Pipeline([('gaussian', GaussianMixture(n_components=3))])\n",
        "\n",
        "pipeline_desempenno.fit(X_pca)\n",
        "\n",
        "labels = pipeline_desempenno.predict(X_pca)\n",
        "\n",
        "# Se grafica\n",
        "fig = px.scatter(\n",
        "    df_pca,\n",
        "    x='PC1',\n",
        "    y='PC2',\n",
        "    color=labels,\n",
        "    title='Clusters utilizando Gaussian Mixture Model'\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Respuesta 6.2:** Al observar los 3 clusters generados en el gr√°fico, se puede notar de manera clara cada grupo de puntos, sin encontrar puntos de cierto cluster en otro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se obtienen estadisticas de los 3 clusters\n",
        "df_pca['labels'] = labels\n",
        "df_pca\n",
        "\n",
        "df_pca.groupby('labels').agg(['mean', 'std'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Respuestas 6.3:** Se puede notar que se presenta una menor desviaci√≥n para el cluster asociado al label 2, pero similares para los labels 0 y 1. Por otro lado, el promedio del label 0 se caerca a valores 0.1 en PC1 y -0.05, pero para el label 2 se presenta un comportamiento opuesto. Luego, el promedio asociado al label 1 es bastante peque√±o en ambos caso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se grafica en 3 dimensiones\n",
        "pipeline = Pipeline([\n",
        "    ('pca', PCA(n_components=3))\n",
        "])\n",
        "\n",
        "# Aplicamos el pipeline\n",
        "X_pca = pipeline.fit_transform(df_vuelos_escalados)\n",
        "\n",
        "# DataFrame con los componentes principales\n",
        "df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2', 'PC3'])\n",
        "fig = px.scatter_3d(\n",
        "    df_pca, \n",
        "    x='PC1', \n",
        "    y='PC2', \n",
        "    z ='PC3',\n",
        "    title='Distribuci√≥n en componentes principales (PCA)',\n",
        "    labels={'PC1': 'Componente Principal 1', 'PC2': 'Componente Principal 2', 'PC3': 'Componente Principal 3'},\n",
        "    color=labels \n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Respuesta 6.5:** Al proyectar los datos a 3 dimensiones utilizando PCA, se puede notar que al igual que en 2 dimensiones, los clusters se diferencian de manera clara unos con otros. Adem√°s, se puede notar que la forma de los datos se mantiene y la divisi√≥n es similar, con una cluster con una mayor cantidad de puntos en comparaci√≥n a los otros 2. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mucho √©xito!\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/55/3d/42/553d42bea9b10e0662a05aa8726fc7f4.gif\" width=300>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "7cb425aec99b4079954fd707109c42c3",
    "deepnote_persisted_session": {
      "createdAt": "2024-04-26T06:15:51.197Z"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
