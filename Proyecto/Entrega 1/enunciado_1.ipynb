{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos**\n",
    "\n",
    "### üë®‚Äçüè´üë©‚Äçüè´ Cuerpo Docente:\n",
    "\n",
    "- Profesor: Sebasti√°n Tinoco, Stefano Schiappacasse\n",
    "- Auxiliar: Melanie Pe√±a Torres, Valentina Rojas Osorio\n",
    "- Ayudante: Valentina Zu√±iga, √Ångelo Mu√±oz \n",
    "\n",
    "### üë®‚Äçüíªüë©‚Äçüíª Estudiantes:\n",
    "- Estudiante n¬∞1: Camila Salas R.\n",
    "- Estudiante n¬∞2: Ignacio Negrete S.\n",
    "\n",
    "_Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Enunciado "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2025-01/proyecto/proyecto.png?raw=true' style=\"border-radius: 12px\"> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el competitivo universo de las bebidas gaseosas, la empresa **SodAI Drinks ü•§** ha logrado destacarse por su creatividad, diversidad de productos y enfoque centrado en el cliente. Ofrece una extensa gama de bebidas carbonatadas que abarca distintos segmentos del mercado: desde productos premium en presentaciones sofisticadas, hasta gaseosas accesibles para el consumo masivo, disponibles en diversos tama√±os y tipos de envases. \n",
    "\n",
    "La compa√±√≠a opera en m√∫ltiples regiones y zonas, sirviendo a una variedad de puntos de venta que incluyen desde tiendas de conveniencia y minimarkets hasta el canal fr√≠o tradicional. Cada tipo de cliente tiene sus particularidades: algunos reciben entregas hasta 4 veces por semana, mientras que otros son visitados por la fuerza de ventas solo una vez semanalmente. Esta diversidad de perfiles representa tanto una oportunidad como un desaf√≠o comercial: ¬øc√≥mo saber qu√© productos tienen m√°s chances de ser comprados por cada cliente en un momento dado?\n",
    "\n",
    "Con el objetivo de aumentar la facturaci√≥n de forma inteligente y mejorar la eficiencia de su estrategia de ventas, **SodAI Drinks** decide crear una nueva c√©lula interna de innovaci√≥n: el equipo **Deep Drinkers ü§ñ**, cuyo prop√≥sito es aplicar ciencia de datos para anticiparse a las necesidades del cliente y potenciar el negocio desde una perspectiva basada en informaci√≥n.\n",
    "\n",
    "El coraz√≥n de esta iniciativa es el desarrollo de un sistema predictivo personalizado para cada cliente. Para ello, **Deep Drinkers** convoca a un equipo de Data Scientists y especialistas en *machine learning* con una misi√≥n clara: construir un modelo predictivo que, cada semana, pueda estimar la probabilidad de compra de cada producto del portafolio para cada cliente activo.\n",
    "\n",
    "El modelo deber√° tener en cuenta m√∫ltiples factores, incluyendo:\n",
    "- **Tipo de cliente**, ej. \"TIENDA DE CONVENIENCIA\", \"MINIMARKET\".\n",
    "- **Frecuencia de entregas y visitas**, indicadores del nivel de actividad comercial.\n",
    "- **Ubicaci√≥n geogr√°fica** (por regi√≥n y zona).\n",
    "- **Preferencias hist√≥ricas de consumo**, inferidas por patrones de compra anteriores.\n",
    "- **Caracter√≠sticas del producto**, como marca, categor√≠a, segmento, tipo de envase y tama√±o\n",
    "\n",
    "El objetivo final es que, **cada semana**, se genere una tabla de productos priorizados: para cada cliente, un listado de productos ordenado por su probabilidad estimada de compra. Esta informaci√≥n ser√° enviada al equipo comercial, que podr√° usarla en call center, para incrementar las chances de concretar ventas al ofrecer justo lo que el cliente probablemente quiere comprar.\n",
    "\n",
    "Este proyecto representa un cambio de paradigma en la forma en que **SodAI Drinks** gestiona su fuerza de ventas: de un enfoque reactivo y generalista, a uno proactivo, basado en datos y profundamente personalizado. As√≠, la empresa no solo espera aumentar su rentabilidad, sino tambi√©n construir relaciones m√°s s√≥lidas con sus clientes, ofreci√©ndoles recomendaciones m√°s relevantes y oportunas.\n",
    "\n",
    "Para lograr lo anterior, el equipo **Deep Drinkers** contar√° con los siguientes conjuntos de datos, junto a sus respectivos atributos:\n",
    "\n",
    "- **Datos transaccionales** (`transacciones.parquet`): contiene el historial de compras realizadas por los clientes.\n",
    "\t- `customer_id`: identificador √∫nico del cliente que realiz√≥ la compra.\n",
    "\t- `product_id`: identificador √∫nico del producto comprado.\n",
    "\t- `purchase_date`: fecha en que se realiz√≥ la transacci√≥n.\n",
    "\t- `order_id`: identificar de la orden de su pedido.\n",
    "\t- `payment`\tmonto total pagado por la transacci√≥n.\n",
    "\n",
    "- **Datos de clientes** (`clientes.parquet`): incluye las caracter√≠sticas de cada cliente.\n",
    "\t- `customer_id`: identificador √∫nico del cliente.\n",
    "\t- `region_id`: identificador de la regi√≥n geogr√°fica donde se encuentra el cliente.\n",
    "\t- `customer_type`: tipo de cliente seg√∫n el canal comercial, por ejemplo, ‚ÄúTIENDA DE CONVENIENCIA‚Äù.\n",
    "\t- `Y`: coordenada geogr√°fica de latitud.\n",
    "\t- `X`: coordenada geogr√°fica de longitud.\n",
    "\t- `num_deliver_per_week`: cantidad de entregas semanales que recibe el cliente.\n",
    "\t- `num_visit_per_week`: frecuencia de visitas de la fuerza de ventas por semana.\n",
    "\n",
    "- **Datos de productos** (`productos.parquet`): describe las caracter√≠sticas de los productos del portafolio.\n",
    "\t- `product_id`: identificador √∫nico del producto.\n",
    "\t- `brand`: marca comercial del producto.\n",
    "\t- `category`: categor√≠a general del producto, como ‚ÄúBEBIDAS CARBONATADAS‚Äù.\n",
    "\t- `sub_category`: subcategor√≠a dentro de la categor√≠a principal, por ejemplo, ‚ÄúGASEOSAS‚Äù.\n",
    "\t- `segment`: segmento de mercado al que pertenece el producto, como ‚ÄúPREMIUM‚Äù.\n",
    "\t- `package`: tipo de envase del producto.\n",
    "\t- `size`: tama√±o del producto en litros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Reglas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://media1.tenor.com/m/0Qtv_cQ4ITsAAAAd/necohaus-grey-name.gif\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "El proyecto consta de **dos entregas parciales** y una **entrega final** en donde la primera entrega la idea es poder reflejar lo aprendido durante la primera mitad del curso, que ser√° sobre los contenidos relacionados a *machine learning*, la segunda ser√° sobre los contenidos de la segunda mitad del curso relacionados a *MLOps* y por √∫ltimo la entrega final constar√° de dos partes, donde la primera ser√° relacionada con experimentaci√≥n sobre nuevos datasets que ser√°n disponibilizados durante las √∫ltimas semanas del curso de manera incremental y una segunda parte que ser√° el informe final escrito que deber√° explicar el desarrollo del proyecto completo, como tambien los resultados y an√°lisis de los experimentos realizados sobre los datasets incrementales. La idea es que todo el c√≥digo est√© desarrollado durante las primeras dos entregas y luego en la entrega final s√≥lo se ejecute el c√≥digo sobre nuevos conjuntos de datos.\n",
    "\n",
    "La idea de generar el proyecto por etapas es poder aliviar la carga de trabajo en las √∫ltimas semanas del semestre donde sabemos que est√°n muy cargado con entregas, pruebas y ex√°menes de otros ramos, y as√≠ garantizamos que habiendo la desarrollado las dos primeras entregas parciales, tendr√°n el grueso del proyecto listo para luego experimentar y documentar.\n",
    "\n",
    "---\n",
    "### **Fechas de entrega**\n",
    "- **Entrega parcial 1**: 14 de Mayo\n",
    "- **Entrega parcial 2**: Por definir\n",
    "- **Entrega final**: Por definir\n",
    "\n",
    "---\n",
    "\n",
    "### **Requisitos del proyecto**\n",
    "- **Grupos**: Formar equipos de **2 personas**. No se aceptar√°n trabajos individuales o grupos con m√°s integrantes.\n",
    "- **Consultas**: Cualquier duda fuera del horario de clases debe ser planteada en el foro correspondiente. Los mensajes enviados al equipo docente ser√°n respondidos √∫nicamente por este medio. Por favor, revisen las respuestas anteriores en el foro antes de realizar nuevas consultas.\n",
    "- **Plagio**: La copia o reutilizaci√≥n no autorizada de trabajos de otros grupos est√° **estrictamente prohibida**. El incumplimiento de esta norma implicar√° la anulaci√≥n inmediata del proyecto y una posible sanci√≥n acad√©mica.\n",
    "- **Material permitido**: Pueden usar cualquier material del curso, ya sea notas, lecturas, c√≥digos, o referencias proporcionadas por los docentes, que consideren √∫til para el desarrollo del proyecto.\n",
    "\n",
    "---\n",
    "\n",
    "### **Entregables y etapas**\n",
    "\n",
    "#### **1. Entrega Parcial 1**  \n",
    "- Dispondr√°n de los archivos de datos **productos.parquet**, **clientes.parquet** y **transacciones.parquet** para el modelamiento inicial.  \n",
    "- Utilizar√°n estos archivos para desarrollar lo solicitado para la entrega 1. \n",
    "- En esta etapa, se espera que apliquen todos los conocimientos aprendidos durante la primera parte del curso relacionados con *machine learning*.\n",
    "- **Informe**: No se exige un avance del informe en esta etapa, s√≥lo un notebook con su desarrollo actual, pero se **recomienda comenzar** a redactar el informe final en paralelo para disminuir la carga acad√©mica en las etapas posteriores.  \n",
    "\n",
    "#### **2. Entrega Parcial 2**  \n",
    "- En esta entrega, deber√°n aplicar los conocimientos aprendidos durante la segunda mitad del curso sobre *MLOps*  \n",
    "- Se espera que implementen estos conocimientos para desplegar su modelo elegido en la primera entrega y crear *pipelines* automatizados que simulen un entorno productivo.\n",
    "- **Informe**: similar a la primera etapa, no se exige un avance del informe, pero se **recomienda avanzar con su redacci√≥n** para evitar una acumulaci√≥n de trabajo en la etapa final.  \n",
    "\n",
    "#### **3. Entrega Final**  \n",
    "- En la entrega final, deber√°n realizar dos etapas:\n",
    "\t- La primera etapa es sobre experimentaci√≥n utilizando datasets incrementales que se ir√°n disponibilizando de manera parcial, para que vayan generando predicciones con su modelo ya desplegado. El objetivo de esta etapa es poder testear su soluci√≥n *end-to-end* y que vayan analizando los resultados obtenidos a medida que se van agregando m√°s datos.\n",
    "\t- La segunda etapa consiste en redactar un informe final que deber√° explicar el desarrollo completo de tu proyecto y un an√°lisis profundo de sus resultados de experimentaci√≥n. Este informe debera incluir a lo menos las siguientes secciones:\n",
    "\t\t- An√°lisis exploratorio de datos  \n",
    "\t\t- Metodolog√≠a aplicada  \n",
    "\t\t- Selecci√≥n y entrenamiento de modelos  \n",
    "\t\t- Evaluaci√≥n de resultados  \n",
    "\t\t- Optimizaci√≥n de modelos\n",
    "\t\t- Interpretabilidad\n",
    "\t\t- Re-entrenamiento\n",
    "\t\t- Tracking con MLFlow\n",
    "\t\t- Creaci√≥n de la aplicaci√≥n web con Gradio y FastAPI\n",
    "\n",
    "Es **altamente recomendable** ir redactando el informe en paralelo al desarrollo de los modelos para garantizar que toda la informaci√≥n relevante quede documentada adecuadamente.  \n",
    "\n",
    "### Nota Final\n",
    "\n",
    "La calificaci√≥n final de su proyecto se calcular√° utilizando la siguiente ponderaci√≥n: \n",
    "\n",
    "$$Nota Final = 0.30 * EntregaParcial1 + 0.40 * EntregaParcial2 + 0.30 * EntregaFinal$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Instrucciones importantes**\n",
    "\n",
    "1. **Formato del informe**:  \n",
    "   - El informe debe estar integrado dentro de un **Jupyter Notebook**. No es necesario subirlo a una plataforma externa, pero debe cumplir con los siguientes requisitos:  \n",
    "     - Estructura clara y ordenada.  \n",
    "     - C√≥digo acompa√±ado de explicaciones detalladas.  \n",
    "     - Resultados presentados de forma visual y anal√≠tica.  \n",
    "\n",
    "2. **Descuento por informes deficientes**:  \n",
    "   - Cualquier secci√≥n del informe que no tenga una explicaci√≥n adecuada o no respete el formato ser√° penalizada con un descuento en la nota. Esto incluye c√≥digo sin comentarios o an√°lisis que no sean coherentes con los resultados presentados.\n",
    "   - Comentarios sin formatear de ChatGPT o herramientas similares ser√°n penalizados (e.g: \"Inserta tu modelo ac√°\", etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì¨ Entrega Parcial 1 (30% del Proyecto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì™ Fecha de Entrega: 14 de Mayo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Abstract [0.25 puntos]\n",
    "\n",
    "<center>\n",
    "<img src=\"https://i.redd.it/h5ptnsyabqvd1.gif\" width=\"400\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta secci√≥n, deben redactar un Abstract claro y conciso para su proyecto. El Abstract debe responder a las siguientes preguntas clave:\n",
    "\n",
    "- **Descripci√≥n del problema**: ¬øCu√°l es el objetivo del proyecto? ¬øQu√© se intenta predecir o analizar?\n",
    "- **Datos de entrada**: ¬øQu√© datos tienen disponibles? ¬øCu√°les son sus principales caracter√≠sticas?\n",
    "- **M√©trica de evaluaci√≥n**: ¬øC√≥mo medir√°n el desempe√±o de sus modelos? Expliquen por qu√© eligieron esta m√©trica bas√°ndose en el an√°lisis exploratorio de los datos.\n",
    "- **Modelos y transformaciones**: ¬øQu√© modelos utilizar√°n y por qu√©? ¬øQu√© transformaciones o preprocesamientos aplicaron a los datos?\n",
    "- **Resultados generales**: ¬øEl modelo final cumpli√≥ con los objetivos del proyecto? ¬øCu√°les fueron las conclusiones m√°s importantes?\n",
    "\n",
    "**Importante**: Escriban esto despues de haber resuelto el resto de la tarea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [Escriba aqu√≠ su Abstract]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Pre-procesamiento [0.5 puntos]\n",
    "\n",
    "<center>\n",
    "<img src=\"https://media0.giphy.com/media/10zsjaH4g0GgmY/giphy.gif?cid=6c09b9523xtlunksc9amikw09zk1bmiqwjqnt70ae82rk877&ep=v1_gifs_search&rid=giphy.gif&ct=g\" width=\"400\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como en muchos otros problemas de negocio, los datos probablemente deben ser pre procesados antes de aplicar cualquier t√©cnica de anal√≠tica. Bajo esa premisa, en esta secci√≥n deben desarrollar c√≥digo que les permita **preparar los datos** de tal forma que les permita resolver el problema planteado. Para esto, pueden aplicar procesamientos como:\n",
    "\n",
    "- Transformaciones de tipo de dato (str, int, etc)\n",
    "- Cruce de informaci√≥n\n",
    "- Eliminaci√≥n de duplicados\n",
    "- Filtros de fila y/o columnas\n",
    "\n",
    "*Hint: ¬øQu√© forma deber√≠a tener la data para resolver un problema de aprendizaje supervisado?*\n",
    "\n",
    "Todo proceso llevado a cabo debe estar bien documentado y justificado en el informe, explicando el por qu√© se decidi√≥ realizar en funcion de los datos presentados y los objetivos planteados del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerias que vamos a usar en la seccion\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, FunctionTransformer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nuestros datasets\n",
    "clientes  = pd.read_parquet('clientes.parquet')\n",
    "productos = pd.read_parquet('productos.parquet')\n",
    "transacciones = pd.read_parquet('transacciones.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "productos.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transacciones.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que podemos notar es que tenemos valores nulos en el dataset. Luego la idea ahora es ampliar la base de transacciones, a√±adiendo la informacion correspondiente al cliente que reliza la transaccion y al producto transado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(transacciones,productos,how='left',on='product_id')\n",
    "df = pd.merge(df,clientes,how='left',on='customer_id')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuacion eliminamos registros duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos la base completa nos gustaria hacer un visualizacion de esta data, para saber si es necesario eliminar columnas irrelevantes o realizar algunas transformaciones. Luego dicha visualizacion puede ser complementada con un analisis por quartiles para enriquecer la exploracion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ocupamos la funcion del lab4\n",
    "# Se observa la distribuci√≥n de las distintas variables\n",
    "columns = df.drop(columns=['customer_id','product_id','order_id']).columns\n",
    "fig = make_subplots(rows=5, cols=3, subplot_titles=columns)\n",
    "\n",
    "# Se agregan las distribuciones\n",
    "for i in range(len(columns)):\n",
    "    fig.add_trace(go.Histogram(x=df[columns[i]]), row=i//3 + 1, col=i%3 + 1)\n",
    "\n",
    "fig.update_layout(height=1000, width=1000, title_text=\"Distribuci√≥n de variables num√©ricas\")\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['size'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['customer_id','product_id','order_id']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos un analisis por cada variable:\n",
    "\n",
    "* **purchase date**: Se observa una cierta estacionalidad con tendencia en la cantidad de transacciones realizadas por fecha\n",
    "\n",
    "* **items**: La cantidad de items se ve concentrada entre 1 y 3 items sin embargo hay claras se√±ales de outliers que se pueden ver tanto en el grafico de distribuciones como en los quartiles\n",
    "\n",
    "* **brand**: La gran mayoria de transacciones se concentran en 6 marcas de un total de 21 marcas\n",
    "\n",
    "* **category**: Todas las transacciones pertenecen a la categoria de bebidas carbonatadas, por tanto esta no es una variable relevante para generar predicciones posteriormente. \n",
    "\n",
    "* **sub category**: Solo tenemos 2 subgcaegorias y la gran mayoria de transacciones pertenecen a gaseosas, aguas saborizadas solo representan el 0,17% de las transacciones.\n",
    "\n",
    "* **segment**: Los segmentos se ven liderados por la categoria medium, luego el resta de categorias son relativamente uniformes en distribucion. \n",
    "\n",
    "* **package**: Tenemos 3 categorias y practicamente todas las transacciones estan concentradas en botellas y latas, Keg solo representa el 0.3% de las transacciones.\n",
    "\n",
    "* **size**: La gran mayoria de las transacciones estan concentradas en el rango [0.25,1], pero tal como se puede ver en los rangos quartilicos y en el grafico de distribucion, hay outliers que no nos permiten obsvervar correctamente la distribucion. Sin embargo a pesar de ser una variable numerica, solo existen 7 tipos de tama√±os lo que nos invita a tratar esta variable como una categorica.\n",
    "\n",
    "* **region id**: Todas las transacciones corresponden a la misma region, que es la region 80. Es por esta razon que para efecto de las predicciones no es una variable relevante. \n",
    "\n",
    "* **zone id**: Al igual que la region, todas las transacciones son realizadas en la misma zona (5148). No es una variable relevante para realizar predicciones. \n",
    "\n",
    "* **customer type**: Tenemos 7 categorias, luego gran parte de las transacciones corresponden a abarrotes. \n",
    "\n",
    "* **X,Y**: Referentes a las coordenadas de la ubicacion de la tienda. Todas las tiendas parecen concentrarse en una region determinada ([-46.5,-46.3],[-108.6,-107.8]), por la naturaleza de los outliers en cada columna, de la impresion de que los outliers son errores de typeo donde las coordenadas X e Y estan invertidas (Posteriormente podemos hacer un analisis para confirmar esta hipotesis).\n",
    "\n",
    "* **num deliver per week**: El numero de entregas por semana se encuentra en el rango [2,6], donde la mayoria de los clientes asociadas a las transacciones reciben 3 entregas\n",
    "\n",
    "* **num visit per week**: Todos los clientes asociados a las transacciones tienen 1 visita semanal a la fuerza de venta, por tanto no es una varible relevante a la hora de hacer predicciones.\n",
    "\n",
    "Una vez hecho este analisis procedemos a eliminar las variables que no nos resultan relevantes a la hora de hacer predicciones\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['category','region_id','zone_id','num_visit_per_week'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå EDA [0.5 puntos]\n",
    "\n",
    "<center>\n",
    "<img src=\"https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExbHZ6aGdkd21tYTI3cW8zYWhyYW5wdGlyb2s3MmRzeTV0dzQ1NWlueiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3k1hJubTtOAKPKx4k3/giphy.gif\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta secci√≥n, se debe realizar un an√°lisis exploratorio de los datos para comprender su estructura, detectar posibles problemas y obtener informaci√≥n relevante para el entrenamiento de los modelos. La idea es que puedan detectar **patrones en los datos** que les permitan resolver el problema con mayor facilidad.\n",
    "\n",
    "Se deben responder preguntas a partir de lo que puedan visualizar/obtener, por ejemplo:\n",
    "\n",
    "- Clientes y productos\n",
    "\n",
    "    - ¬øCu√°ntos clientes √∫nicos hay en el dataset?\n",
    "\n",
    "    - ¬øCu√°ntos productos √∫nicos se encuentran en los datos?\n",
    "\n",
    "- Periodo y frecuencia\n",
    "\n",
    "    - ¬øDe qu√© periodo es la informaci√≥n disponible?\n",
    "\n",
    "    - ¬øCu√°l es la frecuencia de los registros (diaria, semanal, mensual, etc.)?\n",
    "\n",
    "- Calidad de los datos\n",
    "\n",
    "    - ¬øExisten valores nulos en el dataset? ¬øCu√°ntos? ¬øC√≥mo se pueden tratar?\n",
    "\n",
    "    - ¬øHay datos raros, como cantidades negativas o inconsistencias? Genere tests de validaci√≥n para identificar estos problemas.\n",
    "\n",
    "- Patrones de compra\n",
    "\n",
    "    - ¬øCu√°ntos productos compra en promedio cada cliente semana a semana?\n",
    "\n",
    "    - ¬øCu√°ntas transacciones ha realizado cada cliente?\n",
    "\n",
    "    - ¬øCu√°l es el periodo de recompra promedio de cada SKU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clientes, productos y periodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes_unicos = df['customer_id'].nunique()\n",
    "productos_unicos = df['product_id'].nunique()\n",
    "fecha_min = df['purchase_date'].min().strftime('%d/%m/%Y') #Esto es para que no salga la hora en la tabla\n",
    "fecha_max = df['purchase_date'].max().strftime('%d/%m/%Y')\n",
    "periodo = f\"{fecha_min} - {fecha_max}\"\n",
    "\n",
    "fig = go.Figure(data=[go.Table(\n",
    "    header=dict(\n",
    "        values=[\"M√©trica\", \"Valor\"],\n",
    "        fill_color='paleturquoise',\n",
    "        align='left',\n",
    "        font=dict(size=14, color='black')\n",
    "    ),\n",
    "    cells=dict(\n",
    "        values=[\n",
    "            [\"Clientes √∫nicos\", \"Productos √∫nicos\", \"Per√≠odo de registros\"],\n",
    "            [clientes_unicos, productos_unicos, periodo]\n",
    "        ],\n",
    "        fill_color='lavender',\n",
    "        align='left',\n",
    "        font=dict(size=13)\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(title=\"Resumen de los datos\", width=600, height=300)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la tabla anterior podemos ver en primer lugar que no todos los clientes en nuestra base de datos hacen transacciones y que no todos los productos en nuestro catalogo son transados, pues en nuestra base de datos hay 1569 clientes y 971 productos. Luego el periodo de transacciones observado es de 1 a√±o. En cuanto a lo anterior es razonable querer a√±adir estos clientes - productos pues estos productos - clientes pueden compartir muchas similitudes con los que efectivamente se ve en el dateset. Esto generar√≠a que el modelo asocie esos productos a no compra aun cuando si lo pueden ser. Y como la gran mayoria de los productos no son transados las probabilidades disminuiran mucho. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupaciones por frecuencia\n",
    "por_dia = df.groupby(df['purchase_date'].dt.date).size()\n",
    "por_semana = df.groupby(pd.Grouper(key='purchase_date', freq='W')).size()\n",
    "por_mes = df.groupby(pd.Grouper(key='purchase_date', freq='M')).size()\n",
    "\n",
    "# los 3 subplots\n",
    "fig = make_subplots(rows=3, cols=1, shared_xaxes=False,\n",
    "                    subplot_titles=(\"Frecuencia diaria\", \"Frecuencia semanal\", \"Frecuencia mensual\"))\n",
    "\n",
    "# Histograma diario\n",
    "fig.add_trace(go.Bar(x=por_dia.index, y=por_dia.values, name='Diario'),\n",
    "              row=1, col=1)\n",
    "\n",
    "# Histograma semanal\n",
    "fig.add_trace(go.Bar(x=por_semana.index, y=por_semana.values, name='Semanal'),\n",
    "              row=2, col=1)\n",
    "\n",
    "# Histograma mensual\n",
    "fig.add_trace(go.Bar(x=por_mes.index, y=por_mes.values, name='Mensual'),\n",
    "              row=3, col=1)\n",
    "\n",
    "fig.update_layout(height=900, width=1000, title_text=\"Frecuencia de registros por d√≠a, semana y mes\")\n",
    "fig.update_xaxes(title_text=\"Fecha\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Cantidad de registros\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Cantidad de registros\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Cantidad de registros\", row=3, col=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que podemos analizar del grafico anterior es que hay estacionalidad semanal, es decir hay dias de la semana en particular donde probablemente se hacen los pedidos, luego cuando vemos la frecuencia a nivel de semana y mes es claro ver que hay periodos de tiempo en particular (Junio Y Agosto) donde las transacciones caen bastante, y periodos previos a navidad - a√±o nuevo donde las transacciones aumentan mucho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calidad de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos en la seccion anterior no hay valores faltantes, sin embargo si hay anomalias en los datos, en *items* y *size* hay valores atipicos mientras que en las coordenadas X e Y puden haber errores de typeo u outliers, en el caso de *items* y *size* podemos hacer una normalizacion, estandarizacion de los datos o eliminarlos. Mientras que para las coordenadas X e Y podemos hacer un analisis de aquellas tiendas que tienen valores atipicos para ver a que cliente en particular pertenencen y discernir si corresponden a ubicaciones reales o errores de typeo. Finalmente mencionar que en *items* no solo tenemos valores atipicos, si no que tambien negativos, esto no es tan extra√±o pues los valores negativos pueden corresponder a devoluciones, este es un punto sumamente importante pues el objetivo del proyecto es predecir probabilidades de compra. ¬øNos servir√° un cliente que compra 100 y devuelve 90? Mas aun, ¬øQueremos ofrecerle 100 a alguien que nos devolver√° 90? Quizas lo mejor ser√≠a que ese cliente se considere como uno que simplemente compra 10. Lo que sugiere este razonamiento es agrupar las compras y devoluciones de un mismo producto para cada cliente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos en la seccion anterior no tenemos valores faltantes en nuestro dataset, sin embargo no olvidemos que el objetivo de este proyecto es hacer un sistema de recomendacion basado en probabilidades de compra, cual es nuestra variable a predecir y? Son nuestros registros, en efecto cada registro corresponde a una compra y los registros que no corresponden a compras son los registros que no existen en nuestro datset. Como hemos visto en la parte anterior tenemos 1490 tiendas, 114 clientes y 366 dias. Como nosotros queremos estudiar la probabilidad de que un cliente compre un determinado producto en una fecha dada, necesitamos la combinacion de cliente, producto, dia para cada combinacion de estos valores. Esto nos da un total de 62.168.760 registros y sin embargo tenemos tan solo 254.051. Esto quiere decir que tenemos 61.914.709 registros en donde no hay compras. Si hacemos la ampliacion del dataset estos registros donde no hay compras tendrian en sus columnas *order_id, items* valores nulos. Que hacemos con estos valores nulos? A la columna items le asignamos el valor 0, pues decir que no existi√≥ la transaccion es equivalente a decir que se transaron 0 items. Por otra parte podemos asigar un unico order_id a todos los registros inexistentes. Finalmente agregar que queremos hacer predicciones semanales, por tanto se deben agregar los datos a esta temporalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  variable de semana (a√±o-semana) por si despues entregan otro a√±o\n",
    "transacciones['year_week'] = transacciones['purchase_date'].dt.strftime('%Y-%U')  # Semana del a√±o\n",
    "\n",
    "# agregaci√≥n semanal\n",
    "cols_to_keep = ['customer_id', 'product_id', 'year_week']\n",
    "\n",
    "\n",
    "weekly_df = transacciones.groupby(cols_to_keep).agg({\n",
    "    'items': 'sum',\n",
    "}).reset_index()\n",
    "\n",
    "# los valores √∫nicos observados en las transacciones y que por tanto hay que sacar de df y no de las BD originales\n",
    "customers = df['customer_id'].unique()\n",
    "products = df['product_id'].unique()\n",
    "weeks = df['purchase_date'].dt.strftime('%Y-%U').unique()\n",
    "\n",
    "# combinaciones posibles cliente-producto-semana\n",
    "full_index = pd.MultiIndex.from_product(\n",
    "    [customers, products, weeks],\n",
    "    names=['customer_id', 'product_id', 'year_week']\n",
    ")\n",
    "full_df = pd.DataFrame(index=full_index).reset_index()\n",
    "\n",
    "# se une con el dataset agregado\n",
    "df_completo = pd.merge(\n",
    "    full_df,\n",
    "    weekly_df,\n",
    "    on=['customer_id', 'product_id', 'year_week'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# se completa con las variables de clientes y productos\n",
    "df_completo = df_completo.merge(clientes, on='customer_id', how='left')\n",
    "df_completo = df_completo.merge(productos, on='product_id', how='left')\n",
    "\n",
    "df_completo['items'] = df_completo['items'].fillna(0)\n",
    "\n",
    "# Nuestra variable de interes, aqui en particular puede que haya existido un registro y que tenga items 0\n",
    "# aqui lo estamos considerando como no compra\n",
    "df_completo['y'] = (df_completo['items'] > 0).astype(int)\n",
    "\n",
    "#Finalemente esto es para definir la variable temporal como una de tipo datetime y no un string\n",
    "df_completo['year_week'] = pd.to_datetime(df_completo['year_week'] + '-1', format='%Y-%U-%w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patrones de compra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# promedio de productos por semana por cliente\n",
    "promedio_productos_semana = (\n",
    "    df_completo.groupby('customer_id')['items'].mean().reset_index(name='prom_produc_semana')\n",
    ")\n",
    "\n",
    "df_promedio_productos = promedio_productos_semana.reset_index()\n",
    "\n",
    "# transacciones por cliente\n",
    "transacciones = df[['customer_id', 'order_id']].drop_duplicates()\n",
    "transacciones_por_cliente = (\n",
    "    transacciones.groupby('customer_id').size().reset_index(name='transaction_count')\n",
    ")\n",
    "\n",
    "df_transacciones_cliente = transacciones_por_cliente.reset_index()\n",
    "\n",
    "#A√±adimos esta features a nuestro dataframe\n",
    "df_completo = df_completo.merge(promedio_productos_semana, on='customer_id', how='left')\n",
    "df_completo = df_completo.merge(transacciones_por_cliente, on='customer_id', how='left')\n",
    "\n",
    "\n",
    "# la subfigura\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Promedio de productos por semana', 'Transacciones por cliente'),\n",
    ")\n",
    "\n",
    "# Gr√°fico para \"Promedio de productos por semana\"\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df_promedio_productos.index,\n",
    "    y=df_promedio_productos['prom_produc_semana'],\n",
    "    marker=dict(color='skyblue', line=dict(width=2, color='lightblue')),\n",
    "    name='Promedio de productos',\n",
    "), row=1, col=1)\n",
    "\n",
    "# Gr√°fico para \"N√∫mero de transacciones por cliente\"\n",
    "fig.add_trace(go.Bar(\n",
    "    x=df_transacciones_cliente.index, \n",
    "    y=df_transacciones_cliente['transaction_count'], \n",
    "    marker=dict(color='lightcoral', line=dict(width=2, color='lightblue')),\n",
    "    name='Transacciones por cliente',\n",
    "), row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Promedio de productos y transacciones por cliente\",\n",
    "    plot_bgcolor='white',  \n",
    "    yaxis_title='Promedio de productos',\n",
    "    xaxis=dict(tickangle=45),\n",
    "    yaxis=dict(title='Promedio de productos'),  \n",
    "    yaxis2=dict(title='N√∫mero de transacciones', \n",
    "                overlaying='y',  \n",
    "                side='right'), \n",
    "    bargap=0.1,  \n",
    "    showlegend=False \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De los graficos anteriores se puede ver en primer lugar que la mayoria de los clientes tiene un comportamiento parecido en la cantidad de productos que compran semanalmente, a excepcion de un par de clientes que se pueden estar viendo influenciados por los outliers que vimos en la seccion anterior, luego en cuanto a la cantidad de transacciones realizadas la mayor cantidad de clientes se comportan de manera similar sin embargo se pueden distinguir dos grupos, los que realizan alrededor de 50 transacciones y  los que realizan 100 transacciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "\n",
    "df_sorted = df.sort_values(['product_id', 'purchase_date'])\n",
    "\n",
    "df_sorted['recompra_promedio'] = df_sorted.groupby('product_id')['purchase_date'].diff().dt.days\n",
    "\n",
    "recompra_promedio = df_sorted.groupby('product_id')['recompra_promedio'].mean()\n",
    "\n",
    "#A√±adimos la variable al dataset\n",
    "df_completo = df_completo.merge(recompra_promedio, on='product_id', how='left')\n",
    "\n",
    "recompra_promedio_reset = recompra_promedio.reset_index()\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=1, \n",
    "    subplot_titles=('Recompra promedio por SKU',)\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=recompra_promedio_reset.index, \n",
    "    y=recompra_promedio_reset['recompra_promedio'],\n",
    "    marker=dict(color='lightblue', line=dict(width=2, color='lightblue')),\n",
    "    name='Recompra promedio',\n",
    "), row=1, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Recompra promedio por SKU\",\n",
    "    plot_bgcolor='white', \n",
    "    xaxis_title='√çndice de producto',\n",
    "    yaxis_title='Promedio de d√≠as entre compras', \n",
    "    xaxis=dict(tickangle=45),\n",
    "    yaxis=dict(title='Promedio de d√≠as entre compras'),  \n",
    "    bargap=0.1, \n",
    "    showlegend=False \n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este grafico se puede ver que la mayoria de los productos tienen una alta rotacion, a excepcion de un par que no se compran mucho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Holdout [0.25 puntos]\n",
    "\n",
    "Para evaluar correctamente el modelo y garantizar su capacidad de generalizaci√≥n, se deben dividir los datos en tres conjuntos: \n",
    "- `Entrenamiento` : Para ajustar los par√°metros.\n",
    "- `Validaci√≥n`: Para optimizar hiperpar√°metros y seleccionar el mejor modelo.\n",
    "- `Prueba` : Para evaluar el rendimiento final en datos no vistos.\n",
    "\n",
    "üëÄ **Hint**: *Recuerde que los datos tienen una temporalidad que debe considerarse al momento de separarlos, para evitar fugas de informaci√≥n. Es importante justificar la estrategia de partici√≥n elegida y visualizar la distribuci√≥n temporal de los conjuntos generados*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui la estrategia es no usar un train test split aleatorio, pues estariamos mezclando eventos pasados con eventos futuros, por tanto la idea es ordenar por fecha el df y luego dividir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completo = df_completo.sort_values('year_week')\n",
    "\n",
    "n = len(df_completo)\n",
    "train_end = int(n * 0.6)\n",
    "val_end = int(n * 0.8)\n",
    "\n",
    "df_train = df_completo.iloc[:train_end]\n",
    "df_val = df_completo.iloc[train_end:val_end]\n",
    "df_test = df_completo.iloc[val_end:]\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fecha_entrenamiento = df_train['year_week'].max().to_pydatetime()\n",
    "fecha_validacion = df_val['year_week'].max().to_pydatetime()\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_completo['year_week'],\n",
    "    y=df_completo['items'],\n",
    "    mode='lines',\n",
    "    name='Total',\n",
    "    line=dict(color='blue'),\n",
    "    opacity=0.3\n",
    "))\n",
    "\n",
    "fig.add_shape(\n",
    "    type='line',\n",
    "    x0=fecha_entrenamiento,\n",
    "    y0=0,\n",
    "    x1=fecha_entrenamiento,\n",
    "    y1=df_completo['items'].max(),\n",
    "    line=dict(color='green', dash='dash'),\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=fecha_entrenamiento,\n",
    "    y=df_completo['items'].max(),\n",
    "    text=\"Entrenamiento\",\n",
    "    showarrow=True,\n",
    "    arrowhead=1,\n",
    "    ax=0,\n",
    "    ay=-40\n",
    ")\n",
    "\n",
    "fig.add_shape(\n",
    "    type='line',\n",
    "    x0=fecha_validacion,\n",
    "    y0=0,\n",
    "    x1=fecha_validacion,\n",
    "    y1=df_completo['items'].max(),\n",
    "    line=dict(color='orange', dash='dash'),\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=fecha_validacion,\n",
    "    y=df_completo['items'].max(),\n",
    "    text=\"Validaci√≥n\",\n",
    "    showarrow=True,\n",
    "    arrowhead=1,\n",
    "    ax=0,\n",
    "    ay=-40\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Partici√≥n temporal de los datos\",\n",
    "    xaxis_title=\"Fecha\",\n",
    "    yaxis_title=\"Cantidad de √≠tems\",\n",
    "    template=\"plotly_white\",\n",
    "    width=1000,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Feature Engineering [0.5 puntos]\n",
    "\n",
    "<center>\n",
    "<img src=\"https://i.imgur.com/CmXZSSC.gif\" width=\"300\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta secci√≥n, se deben construir pipelines para automatizar el preprocesamiento de los datos, lo cual garantizar√° que el flujo de trabajo sea reproducible y eficiente para esta entrega y las futuras. El objetivo es aplicar una serie de transformaciones en un orden definido para asegurar que los datos est√©n listos para los modelos a entrenar. El pipeline final debe incluir las t√©cnicas de pre-procesamiento que se deben aplicar a los distintos datos (seg√∫n lo que consideren necesario para el problema). Por ejemplo:\n",
    "\n",
    "- **Imputaci√≥n de valores nulos**: Manejo de datos faltantes mediante estrategias adecuadas (media, mediana, moda, interpolaci√≥n, etc.). \n",
    "\n",
    "- **Transformaciones personalizadas**: Uso de ColumnTransformer para aplicar diferentes transformaciones a columnas espec√≠ficas.\n",
    "\n",
    "- **Codificaci√≥n de variables categ√≥ricas**: Convertir datos categ√≥ricos a un formato num√©rico adecuado (One-Hot Encoding, Label Encoding, etc.).\n",
    "\n",
    "- **Discretizaci√≥n de variables**: Conversi√≥n de variables num√©ricas continuas en categor√≠as si son relevantes para el desempe√±o del modelo a entrenar.\n",
    "\n",
    "- **Estandarizaci√≥n o normalizaci√≥n** : Ajustar la escala de los datos para mejorar el rendimiento de los algoritmos sensibles a la magnitud de las variables.\n",
    "\n",
    "- **Eliminaci√≥n o transformaci√≥n de valores at√≠picos**: Identificar y tratar con datos outliers para mejorar la robustez del modelo.\n",
    "\n",
    "- **Nuevas caracter√≠sticas** : Creaci√≥n de variables adicionales que puedan aportar informaci√≥n relevante al modelo.\n",
    "\n",
    "Cada una de estas transformaciones debe ser justificada en funci√≥n de su relevancia para el problema y los datos, y es importante evaluar su impacto en el rendimiento del modelo. Adem√°s, el pipeline debe ser flexible y modular para poder probar diferentes configuraciones de preprocesamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purchase Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateParser(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        fecha = pd.to_datetime(X.iloc[:, 0], format='%Y-%U-%w')\n",
    "        return pd.DataFrame({\n",
    "            'week': fecha.dt.isocalendar().week.astype('int'),\n",
    "            'month': fecha.dt.month.astype('int'), #igual a√±adimos el mes\n",
    "            'year': fecha.dt.year.astype('int')\n",
    "        }, index=X.index)\n",
    "\n",
    "    def set_output(self, transform='default'):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos anteriormente si bien esta variable es numerica, es de naturaleza categorica, pues clasifica los bebestibles en 7 tama√±os distintos, para hacer un tratamiento de este podemos crear una clase que transforme esta variable en categorica, para luego pasarla a traves de un encoder. Con la exploracion realizada en la seccion de pre-procesamiento sabemos que hay 7 categorias de tama√±os, sin embargo se pueden resumir en 4: peque√±as : $[0.25,0.31,0.33]$; medianas : $[0.5,0.66]$; grandes: $[1]$; muy grandes: $[20]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricaSize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, col='size'):\n",
    "        self.col = col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        def categorizar(val):\n",
    "            if val in [0.25, 0.31, 0.33]:\n",
    "                return 'peque√±a'\n",
    "            elif val in [0.5, 0.66]:\n",
    "                return 'mediana'\n",
    "            elif val == 1:\n",
    "                return 'grande'\n",
    "            elif val == 20:\n",
    "                return 'muy_grande'\n",
    "\n",
    "\n",
    "        X[self.col] = X[self.col].apply(categorizar).astype('category')\n",
    "        return X\n",
    "    \n",
    "    def set_output(self,transform='default'):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X e Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora hagamos el tratamiento de las coordenadas X y Y mencionadas anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['X'] > -100]['customer_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Y'] < -50]['customer_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero aqui es observar que los outliers que vimos en la coordenadas X e Y pertenecen a los mismos clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['customer_id'] == 236766]['Y'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['customer_id'] == 219231]['Y'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['customer_id'] == 165126]['Y'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver todas las tiendas asociadas a esos registros tienen la misma ubicacion, por tanto no podemos ver que existe un ejemplo dentro de esas tiendas donde se vea que las coordenadas estan invertidas. Sin embargo hay algo que si sabemos y es que todas las tiendas se ubican en la misma region y en la misma zona, por tanto debiesen estar en lugares cercanos. Veamos cuanta distancia hay entre los puntos que suponemos que tienen coordenadas invertidas y el resto de los otros puntos para ver si son puntos factibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n Haversine\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Radio de la Tierra en km \n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    delta_phi = np.radians(lat2 - lat1)\n",
    "    delta_lambda = np.radians(lon2 - lon1)\n",
    "    \n",
    "    a = np.sin(delta_phi / 2.0)**2 + \\\n",
    "        np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "# Una tienda normal\n",
    "x1, y1 = -107.125965, -46.489646  # X: longitud, Y: latitud\n",
    "\n",
    "# Una tienda con coordenadas supuestamente invertidas\n",
    "x2, y2 = -46.589622, -107.899346\n",
    "\n",
    "dist_km = haversine(y1, x1, y2, x2)\n",
    "print(f\"Distancia entre puntos: {dist_km:.2f} km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una distancia de 6000 km es mas que la distancia que hay desde Arica hasta Puerto Willams, por tanto en vista de que sabemos que ambos puntos se encuentran en la misma zona, podemos concluir que los puntos estan invertidos. Creemos una funcion que nos permita tener correctamente las variables y luego veamos como se ven en un plano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invertir_coordenadas(df_completo, x_col='X', y_col='Y'):\n",
    "    df_completo = df_completo.copy()\n",
    "\n",
    "    x_valido = df_completo[x_col].between(-110, -106)\n",
    "    y_valido = df_completo[y_col].between(-47, -45)\n",
    "    fuera_de_rango = ~(x_valido & y_valido)\n",
    "\n",
    "    x_invertido_valido = df_completo[y_col].between(-110, -106)\n",
    "    y_invertido_valido = df_completo[x_col].between(-47, -45)\n",
    "    se_pueden_invertir = fuera_de_rango & x_invertido_valido & y_invertido_valido\n",
    "\n",
    "    df_completo.loc[se_pueden_invertir, [x_col, y_col]] = df_completo.loc[se_pueden_invertir, [y_col, x_col]].values\n",
    "\n",
    "    return df_completo\n",
    "\n",
    "df_completo = invertir_coordenadas(df_completo)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.scatter(df_completo['X'], df_completo['Y'], alpha=0.7, c='green', label='Corregido')\n",
    "plt.title('Despu√©s de invertir')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver de forma mucho mas clara la distribucion de las tiendas en el plano. Ahora la implementacion en una clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertirTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, x_col='X', y_col='Y'):\n",
    "        self.x_col = x_col\n",
    "        self.y_col = y_col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        x_valido = X[self.x_col].between(-110, -106)\n",
    "        y_valido = X[self.y_col].between(-47, -45)\n",
    "        fuera_de_rango = ~(x_valido & y_valido)\n",
    "\n",
    "        x_invertido_valido = X[self.y_col].between(-110, -106)\n",
    "        y_invertido_valido = X[self.x_col].between(-47, -45)\n",
    "        se_pueden_invertir = fuera_de_rango & x_invertido_valido & y_invertido_valido\n",
    "\n",
    "        X.loc[se_pueden_invertir, [self.x_col, self.y_col]] = X.loc[se_pueden_invertir, [self.y_col, self.x_col]].values\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def set_output(self,transform='default'):\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos visualizado en la seccion de preprocesamiento, la gran cantidad de items se concentra en valores bajos en efecto mas del 75% de las transacciones tienen asociados menos de 4 items. Sin embargo tenemos transacciones con 200, 300 hasta 1000 items, es razonable pensar que estos no son valores reales y que por tanto deben ser eliminados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZScoreFilter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, col='items', threshold=1.96):\n",
    "        self.col = col\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.mean_ = X[self.col].mean()\n",
    "        self.std_ = X[self.col].std()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        z_scores = (X[self.col] - self.mean_) / self.std_\n",
    "        mask = np.abs(z_scores) <= self.threshold\n",
    "        return X[mask].reset_index(drop=True)\n",
    "    \n",
    "    def set_output(self,transform='default'):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brand cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['brand'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ser la marca una variable categorica, para efectos del modelo tendremos que codificar como una dummy cad uno de sus valores (21). Para reducir la dimensionalidad de las marcas podemos hacer un analisis de clustering sobre las marcas y agrupar aquellas que son parecidas. Primero veamos la eleccion de los clusters y luego una implementacion mas formal para incluirla en el pipeline general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columnas relevantes\n",
    "df_cluster = df_completo[['brand', 'segment','package','size']].copy()\n",
    "\n",
    "# eliminamos duplicados por producto para evitar sesgo por transacci√≥n\n",
    "df_cluster = df_cluster.drop_duplicates(subset=['brand', 'segment', 'package','size'])\n",
    "\n",
    "# brand\n",
    "agg = df_cluster.groupby('brand').agg({\n",
    "    'size' : 'mean',\n",
    "    'package': lambda x: x.mode()[0],\n",
    "    'segment': lambda x: x.mode()[0],\n",
    "}).reset_index()\n",
    "\n",
    "# One-hot encoding para variables categ√≥ricas\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "encoded = ohe.fit_transform(agg[['segment','package']])\n",
    "encoded_df = pd.DataFrame(encoded, columns=ohe.get_feature_names_out(['segment','package']))\n",
    "encoded_df.index = agg.index  # Alinear √≠ndices para concatenaci√≥n\n",
    "\n",
    "# Construcci√≥n del dataset para clustering\n",
    "X = pd.concat([agg[['size']], encoded_df], axis=1)\n",
    "\n",
    "# Clustering con KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "agg['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cluster in agg['cluster'].unique():\n",
    "    plt.scatter(\n",
    "        X_2d[agg['cluster'] == cluster, 0],\n",
    "        X_2d[agg['cluster'] == cluster, 1],\n",
    "        label=f'Cluster {cluster}'\n",
    "    )\n",
    "\n",
    "for i, brand in enumerate(agg['brand']):\n",
    "    plt.text(X_2d[i, 0], X_2d[i, 1], brand, fontsize=8)\n",
    "\n",
    "plt.title(\"Clusters de Marcas (Brand) con `items` incluido\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay un cumulo de tiendas en el sector inferior izquierdo, sumado a peque√±os grupos de tiendas que parecen estar cerca en la esquina superior izquierda, si bien no son clusters claramente definidos tener 3 tiendas relativamente parecidas es mejor que 21 marcas por separado. Ahora podemos hacer la transformacion a traves de una clase para incorporarlo al pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Idem a la parte anterior pero en la clase\n",
    "class BrandClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_clusters=3):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        self.scaler = StandardScaler()\n",
    "        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=0)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        df = X.copy()\n",
    "\n",
    "        df = df.drop_duplicates(subset=['brand', 'segment', 'recompra_promedio'])\n",
    "\n",
    "        # Agrupaci√≥n por brand\n",
    "        agg = df.groupby('brand').agg({\n",
    "            'recompra_promedio': 'mean',\n",
    "            'items': 'mean',\n",
    "            'segment': lambda x: x.mode()[0] if not x.mode().empty else 'otro'\n",
    "        }).reset_index()\n",
    "\n",
    "        self.agg_ = agg.copy()\n",
    "\n",
    "        segment_encoded = self.encoder.fit_transform(agg[['segment']])\n",
    "        segment_df = pd.DataFrame(segment_encoded, columns=self.encoder.get_feature_names_out(['segment']))\n",
    "        segment_df.index = agg.index\n",
    "\n",
    "        X_cluster = pd.concat([agg[['recompra_promedio', 'items']], segment_df], axis=1)\n",
    "\n",
    "        X_scaled = self.scaler.fit_transform(X_cluster)\n",
    "\n",
    "        self.kmeans.fit(X_scaled)\n",
    "\n",
    "        self.brand_to_cluster = dict(zip(agg['brand'], self.kmeans.labels_))\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['brand_cluster'] = X['brand'].map(self.brand_to_cluster)\n",
    "        return X\n",
    "    \n",
    "    def set_output(self, transform='default'):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X e Y cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizamos las variables relacionadas al cliente - tienda para hacer el cluster\n",
    "df_cluster = df_completo[['X', 'Y', 'customer_type', 'num_deliver_per_week', 'num_visit_per_week']].copy()\n",
    "\n",
    "agg = df_cluster.groupby(['X', 'Y']).agg({\n",
    "    'customer_type': lambda x: x.mode()[0] if not x.mode().empty else 'otro',\n",
    "    'num_deliver_per_week': 'mean',\n",
    "    'num_visit_per_week': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "customer_encoded = ohe.fit_transform(agg[['customer_type']])\n",
    "customer_encoded_df = pd.DataFrame(customer_encoded, columns=ohe.get_feature_names_out(['customer_type']))\n",
    "\n",
    "X = pd.concat([\n",
    "    agg[['num_deliver_per_week', 'num_visit_per_week']].reset_index(drop=True),\n",
    "    customer_encoded_df.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# Aplicaci√≥n de DBSCAN\n",
    "dbscan = DBSCAN(eps=0.7, min_samples=80)\n",
    "agg['cluster'] = dbscan.fit_predict(X)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cluster in agg['cluster'].unique():\n",
    "    plt.scatter(\n",
    "        X_2d[agg['cluster'] == cluster, 0],\n",
    "        X_2d[agg['cluster'] == cluster, 1],\n",
    "        label=f'Cluster {cluster}'\n",
    "    )\n",
    "\n",
    "for i, (x, y) in enumerate(X_2d):\n",
    "    plt.text(x, y, f'{agg[\"X\"].iloc[i]}, {agg[\"Y\"].iloc[i]}', fontsize=8)\n",
    "\n",
    "plt.title(\"Visualizaci√≥n de Clusters (X, Y, items, size)\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando una tecnica de clustering basada en densidad podemos encontrar 2 clusters mas un par outliers. Ahora la implementacion en una clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lo mismo pero en la clase\n",
    "class XYClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, eps=0.7, min_samples=80):\n",
    "        self.eps = eps\n",
    "        self.min_samples = min_samples\n",
    "        self.encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        self.dbscan = DBSCAN(eps=self.eps, min_samples=self.min_samples)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        df = X.copy()\n",
    "\n",
    "        agg = df.groupby(['X', 'Y']).agg({\n",
    "            'customer_type': lambda x: x.mode()[0] if not x.mode().empty else 'otro',\n",
    "            'num_deliver_per_week': 'mean',\n",
    "            'num_visit_per_week': 'mean'\n",
    "        }).reset_index()\n",
    "\n",
    "        customer_encoded = self.encoder.fit_transform(agg[['customer_type']])\n",
    "        X_cluster = np.concatenate([\n",
    "            agg[['num_deliver_per_week', 'num_visit_per_week']].values,\n",
    "            customer_encoded\n",
    "        ], axis=1)\n",
    "\n",
    "        self.dbscan.fit(X_cluster)\n",
    "        self.XY_to_cluster = dict(zip(zip(agg['X'], agg['Y']), self.dbscan.labels_))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['XY_cluster'] = X.apply(lambda row: self.XY_to_cluster.get((row['X'], row['Y']), -1), axis=1)\n",
    "        return X\n",
    "\n",
    "    def set_output(self, transform='default'):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PipeLine inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "lista_stand = ['num_deliver_per_week','prom_produc_semana','transaction_count','recompra_promedio']\n",
    "lista_categoricas = ['package', 'customer_type', 'segment','brand_cluster','XY_cluster']\n",
    "lista_temporales = ['year_week']\n",
    "lista_invertir = ['X','Y']\n",
    "lista_convertir = ['size']\n",
    "\n",
    "\n",
    "trans_stand = Pipeline([\n",
    "    ('simple_imputer', SimpleImputer(strategy='mean')), \n",
    "    ('MinMaxScaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "trans_categoricas = Pipeline([\n",
    "    ('OneHotEncoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "trans_temporales = Pipeline([\n",
    "    ('Date_Parser', DateParser())\n",
    "])\n",
    "\n",
    "trans_invertir = Pipeline([\n",
    "    ('Invertir', InvertirTransformer())\n",
    "])\n",
    "\n",
    "trans_convertir = Pipeline([\n",
    "    ('Convertir',CategoricaSize()),\n",
    "    ('OneHotEncoder_size', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocesamiento = Pipeline([\n",
    "    ('inv', InvertirTransformer()),\n",
    "    ('brand_cluster', BrandClusterTransformer(n_clusters=3)),\n",
    "    ('xy_cluster', XYClusterTransformer()),\n",
    "    ('column_transform', ColumnTransformer([\n",
    "        ('std', trans_stand, lista_stand),\n",
    "        ('enc', trans_categoricas, lista_categoricas),\n",
    "        ('tmp', trans_temporales, lista_temporales),\n",
    "        ('con', trans_convertir, lista_convertir)\n",
    "    ]))\n",
    "])  \n",
    "\n",
    "preprocesamiento.set_output(transform='pandas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformado = preprocesamiento.fit_transform(df_completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformado.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Baseline [0.25 puntos]\n",
    "\n",
    "<center>\n",
    "<img src=\"https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExN3lzeGFqZmU3NzJrZHllNjRmaHVzczJpZ29rdHdlMzVpZnQwNXo1diZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/qAtZM2gvjWhPjmclZE/giphy.gif\" width=\"300\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta secci√≥n se debe construir el modelo m√°s sencillo posible que pueda resolver el problema planteado, conocido como **Modelo baseline**. Su prop√≥sito es servir como referencia para comparar el rendimiento de los modelos m√°s avanzados desarrollados en etapas posteriores.  \n",
    "\n",
    "Pasos requeridos:  \n",
    "- Implemente, entrene y eval√∫e un modelo b√°sico utilizando un pipeline.  \n",
    "- Aseg√∫rese de incluir en el pipeline las transformaciones del preprocesamiento realizadas previamente junto con un clasificador b√°sico.  \n",
    "- Eval√∫e el modelo y presente el informe de m√©tricas utilizando **`classification_report`**.  \n",
    "\n",
    "Documente claramente c√≥mo se cre√≥ el modelo, las decisiones tomadas y los resultados obtenidos. Este modelo ser√° la base comparativa en las secciones posteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se comienzan separando los df en X e y, seg√∫n cada caso (train, val y test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se obtiene X e y en train, test y val\n",
    "y_train = df_train['y']\n",
    "X_train = df_train.drop(columns=['y'])\n",
    "\n",
    "y_val = df_val['y']\n",
    "X_val = df_val.drop(columns=['y'])\n",
    "\n",
    "y_test = df_test['y']\n",
    "X_test = df_test.drop(columns=['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Luego, en base a el conocimiento adquirido en laboratorios pasados y sabiendo que Dummy Clussidier es un modelo de clasificaci√≥n simple que genera predicciones en base a la estrategia definida, pero sin aprender de los datos. En particular, la estrategia a seguir es 'stratified' ya que los datos se encuentran desbalanceados, por lo que esto respetar√° este comportamiento, observando as√≠ un modelo simple, el cual idealmente debe ser superado por modelos mucho m√°s complejos en donde en el entrenamiento si aprenden de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Se construye el baseline \n",
    "# Se juntan los pasos en un pipeline y se entrena\n",
    "pipeline_dummy = Pipeline([('preprocesamiento', preprocesamiento),\n",
    "                     ('dummy', DummyClassifier(strategy='stratified'))])\n",
    "\n",
    "pipeline_dummy.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A continuaci√≥n, se obtiene la predicci√≥n del modelo a partir de y_test, obteniendo un reporte del desempe√±o del modelo del baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Se obtiene el clasification_report\n",
    "y_pred = pipeline_dummy.predict(X_test)\n",
    "\n",
    "print('Reporte de desempe√±o del modelo dummy: ')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Observando los resultados obtenidos en las distintas m√©tricas y el conocimiento en el desbalance en las clases, este comportamiento se ve reflejado en la clase mayoritaria con m√©tricas bastante cercanas a 1 en todos los casos, pero si se observa la clase minoritaria, las m√©tricas se acercan bastante a cero, por lo que es necesario encontrar modelos que puedan trabajar con este problema. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Elecci√≥n de modelo [0.75 puntos]\n",
    "\n",
    "En esta secci√≥n deben escoger un modelo que se adapte a las necesidades del negocio. Para esto, pruebe al menos 3 modelos y desarrolle los siguientes aspectos para cada uno:\n",
    "\n",
    "- **Estructura y diferencias entre los modelos**: Explicar brevemente cada uno y sus hip√©rpar√°metros de mayor importancia.\n",
    "- **Clasificadores recomendados**:\n",
    "  - `LogisticRegression`\n",
    "  - `KNeighborsClassifier`\n",
    "  - `DecisionTreeClassifier`\n",
    "  - `SVC`\n",
    "  - `RandomForestClassifier`\n",
    "  - `LightGBMClassifier` (del paquete `lightgbm`)\n",
    "  - `XGBClassifier` (del paquete `xgboost`)\n",
    "  - Otro (seg√∫n lo que se estime adecuado)\n",
    "  \n",
    "- **Evaluaci√≥n de resultados**: Se utilizar√° el **`classification_report`** para evaluar el rendimiento de cada modelo, destacando m√©tricas clave como precisi√≥n, recall y F1-score. **Importante: No optimicen hiperpar√°metros, la idea es hacer una selecci√≥n r√°pida del modelo.**\n",
    "\n",
    "**Nota:** Pueden ocupar mas de 1 **instancia** de modelo para resolver el problema (e.g: (modelo_1, grupo_1), (modelo_2, grupo_2), ...).\n",
    "  \n",
    "A continuaci√≥n, se deben responder las siguientes preguntas para evaluar el rendimiento de los modelos entrenados:\n",
    "\n",
    "1. ¬øHay alg√∫n clasificador que supere al modelo baseline?  \n",
    "2. ¬øCu√°l es el mejor clasificador entrenado y por qu√©?  \n",
    "3. ¬øQu√© factores hacen que el mejor clasificador sea superior a los otros?  \n",
    "4. En t√©rminos de `tiempo de entrenamiento`, ¬øQu√© modelo considera m√°s adecuado para experimentar con grillas de optimizaci√≥n?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En base a los modelos propuestos la elecci√≥n de 3 de estos se decidi√≥ por las siguientes razones:\n",
    "    * KNeighborsClassifier: al trabajar con vecinos puede entenderse mejor el negocio, ya que se pueden agrupar comportamientos similares y as√≠ lograr predecir de mejor manera, pero teniendo en cuenta el tiempo de entrenamiento.\n",
    "    * RandomForestClassifier: es un modelo bastante conocido para entrenar, el cual se conoce por tener un buen desempe√±o en general, debido a el entrenamiento con m√∫ltiples √°rboles, en vez de uno, lo que permite obtener resultados m√°s precisos, pero teniendo en cuenta que este modelo tambi√©n tiene un tiempo alto de entrenamiento.\n",
    "    * XGBoost: teniendo tanto LGBM como XGB se elige el segundo, ya que se obtienen resultados similares en las predicciones, con una leve diferencia en el tiempo de entrenamiento, pero en comparaci√≥n con los elegidos, este es mucho menor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A continuaci√≥n, se explican brevemente los modelos y sus hiperpar√°metros m√°s importantes:\n",
    "    * KNeighborsClassifier:\n",
    "        * Descripci√≥n:\n",
    "        * Hiperpar√°metros:\n",
    "    * RandomForestClassifier:\n",
    "        * Descripci√≥n:\n",
    "        * Hiperpar√°metros:\n",
    "    * XGBoost:\n",
    "        * Descripci√≥n:\n",
    "        * Hiperpar√°metros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se implementa LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Se juntan los pasos en un pipeline y se entrena\n",
    "pipeline_LR = Pipeline([('preprocesamiento', preprocesamiento),\n",
    "                     ('LogisticRegression', LogisticRegression())])\n",
    "\n",
    "pipeline_LR.fit(X_train, y_train)\n",
    "\n",
    "# Se obtiene el clasification_report\n",
    "y_pred_LR = pipeline_LR.predict(X_test)\n",
    "\n",
    "print('Reporte de desempe√±o del modelo Logistic Regression: ')\n",
    "print(classification_report(y_test, y_pred_LR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Seg√∫n las m√©tricas obtenidas para el modelo ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se implementa RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Se juntan los pasos en un pipeline y se entrena\n",
    "pipeline_RFC= Pipeline([('preprocesamiento', preprocesamiento),\n",
    "                     ('random forest classifier', RandomForestClassifier())])\n",
    "\n",
    "pipeline_RFC.fit(X_train, y_train)\n",
    "\n",
    "# Se obtiene el clasification_report\n",
    "y_pred_RFC= pipeline_RFC.predict(X_test)\n",
    "\n",
    "print('Reporte de desempe√±o del modelo Random Forest Classifier: ')\n",
    "print(classification_report(y_test, y_pred_RFC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Seg√∫n las m√©tricas obtenidas para el modelo ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se implementa XGBoost classifier\n",
    "from xgboost import XGBClassifier\n",
    "# Se juntan los pasos en un pipeline y se entrena\n",
    "pipeline_XGBC= Pipeline([('preprocesamiento', preprocesamiento),\n",
    "                     ('xgboost classifier', XGBClassifier())])\n",
    "\n",
    "pipeline_XGBC.fit(X_train, y_train)\n",
    "\n",
    "# Se obtiene el clasification_report\n",
    "y_pred_XGBC= pipeline_XGBC.predict(X_test)\n",
    "\n",
    "print('Reporte de desempe√±o del modelo XGBoost Classifier: ')\n",
    "print(classification_report(y_test, y_pred_XGBC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Seg√∫n las m√©tricas obtenidas para el modelo ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¬øHay alg√∫n clasificador que supere al modelo baseline?  \n",
    "\n",
    "2. ¬øCu√°l es el mejor clasificador entrenado y por qu√©?  \n",
    "\n",
    "3. ¬øQu√© factores hacen que el mejor clasificador sea superior a los otros?  \n",
    "\n",
    "4. En t√©rminos de `tiempo de entrenamiento`, ¬øQu√© modelo considera m√°s adecuado para experimentar con grillas de optimizaci√≥n?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Optimizaci√≥n de Hiperpar√°metros [1.0 puntos]\n",
    "\n",
    "<center>\n",
    "<img src=\"https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExcXJkNzdhYjlneHplaGpsbnVkdzh5dnY3Y2VyaTIzamszdGR1czJ2diZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/2rqEdFfkMzXmo/giphy.gif\" width=\"300\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de su an√°lisis anterior, se debe proceder a optimizar el rendimiento del modelo seleccionado mediante la optimizaci√≥n de sus hiperpar√°metros. Para ello, se espera que implementen `Optuna` para optimizar no solo los hiperpar√°metros del modelo, sino tambi√©n los de los preprocesadores utilizados (por ejemplo, OneHot Encoding, Scalers, etc.).\n",
    "\n",
    "Al desarrollar este proceso, deber√°n responder las siguientes preguntas clave como m√≠nimo:\n",
    "\n",
    "- ¬øQu√© m√©trica decidieron optimizar y por qu√©?\n",
    "\n",
    "- ¬øQu√© hiperpar√°metro tuvo un mayor impacto en el rendimiento de su modelo?\n",
    "\n",
    "- ¬øCu√°nto mejor√≥ el rendimiento del modelo despu√©s de la optimizaci√≥n de hiperpar√°metros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Hiperpar√°metros que se pide optimizar con sus rangos\n",
    "\n",
    "\n",
    "    # Se asignan a los preprocesadores, los hiperparametros a optimizar\n",
    "\n",
    "\n",
    "    # Entrenamos con lo optimizado\n",
    "    model = ...\n",
    "\n",
    "    # Pipeline con el modelo y transformaci√≥n\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocesamiento', preprocesamiento),\n",
    "        ('...', model)\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "    # Guardamos\n",
    "    trial.set_user_attr(\"best_pipeline\", pipeline)\n",
    "\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finamos una semilla para reproducibilidad\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "study.optimize(objective, timeout=300)  # por 5 minutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"N√∫mero de trials:\", len(study.trials))\n",
    "print(\"Mejor MAE:\", study.best_value)\n",
    "print(\"Mejores hiperpar√°metros:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo\n",
    "model_xgb_optuna = study.best_trial.user_attrs[\"best_pipeline\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Interpretabilidad [1.0 puntos]\n",
    "\n",
    "En esta secci√≥n, deben explicar el funcionamiento de su modelo utilizando las t√©cnicas de interpretabilidad vistas en clase, como `SHAP`. Se espera que sean capaces de descomponer las predicciones y evaluar la importancia de los atributos y las interacciones entre ellos, con el fin de obtener una comprensi√≥n m√°s profunda de c√≥mo el modelo toma decisiones. \n",
    "\n",
    "Al desarrollar esta parte, deber√°n responder las siguientes preguntas clave como m√≠nimo:\n",
    "\n",
    "- ¬øPodr√≠a explicar el funcionamiento de su modelo para una predicci√≥n en particular? Si es as√≠, proporcione al menos tres ejemplos espec√≠ficos, describiendo c√≥mo el modelo lleg√≥ a sus decisiones y qu√© factores fueron m√°s relevantes en cada caso.\n",
    "\n",
    "- ¬øQu√© atributo tiene una mayor importancia en la salida de su modelo? Analice si esto tiene sentido con el problema planteado y justifique la relevancia de dicho atributo en el contexto de las predicciones que se realizan.\n",
    "\n",
    "- ¬øExiste alguna interacci√≥n entre atributos que sea relevante para el modelo? Investigue si la combinaci√≥n de ciertos atributos tiene un impacto significativo en las predicciones y expl√≠quela en **detalle**.\n",
    "\n",
    "- ¬øPodr√≠a existir sesgo hacia alg√∫n atributo en particular? Reflexione sobre la posibilidad de que el modelo est√© favoreciendo ciertos atributos. Si es as√≠, ¬øcu√°l podr√≠a ser la causa y qu√© impacto podr√≠a tener esto en la predicci√≥n?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se comenzaran observando las importancias de las features de manera global:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Se obtiene el orden de las features\n",
    "order = pipeline_XGBC.named_steps['xgboost classifier'].get_booster().feature_names\n",
    "\n",
    "# Se implementa un metodo de permutacion para observar la importancia de features\n",
    "result = permutation_importance(pipeline_XGBC, X_test, y_test, n_repeats=1, random_state=42)\n",
    "\n",
    "# Se visualiza en un grafico\n",
    "perm_sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "new_order = []\n",
    "for i in range(perm_sorted_idx.shape[0]-1, -1, -1):\n",
    "    new_order = np.append(new_order, order[perm_sorted_idx[i]])\n",
    "\n",
    "plt.boxplot(\n",
    "    result.importances[perm_sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels = new_order\n",
    ")\n",
    "plt.title(\"Importancia de las Features por Incremento del Error\")\n",
    "plt.xlabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Si se observa el gr√°fico de importancia ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ahora, se intentar√° explicar el funcionamiento del modelo para ciertas predicciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define el explainer del mdelo\n",
    "import shap \n",
    "explainer = shap.TreeExplainer(pipeline_XGBC.named_steps['xgboost classifier'])\n",
    "shap_values = explainer(df_completo.drop(columns=['y']))\n",
    "\n",
    "# transformar logits a probabilidad\n",
    "np.exp(shap_values.base_values) / (1 + np.exp(shap_values.base_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primer ejemplo\n",
    "idx_muestra = 1\n",
    "\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, \n",
    "                shap_values.values[idx_muestra,:], \n",
    "                X.iloc[idx_muestra,:],\n",
    "                link=\"logit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segundo ejemplo\n",
    "idx_muestra = 2\n",
    "\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, \n",
    "                shap_values.values[idx_muestra,:], \n",
    "                X.iloc[idx_muestra,:],\n",
    "                link=\"logit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tercer ejemplo\n",
    "idx_muestra = 3\n",
    "\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, \n",
    "                shap_values.values[idx_muestra,:], \n",
    "                X.iloc[idx_muestra,:],\n",
    "                link=\"logit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Resultados y Conclusiones [1.0 puntos]\n",
    "\n",
    "Para finalizar, se deben explicar los desarrollos y resultados obtenidos a lo largo de todo el proceso, desde la selecci√≥n de las variables hasta la optimizaci√≥n de hiperpar√°metros e interpretaci√≥n. Se espera una reflexi√≥n cr√≠tica sobre el desempe√±o de los modelos entrenados y una comparaci√≥n entre los diferentes enfoques. Adem√°s, deber√°n abordar los siguientes puntos clave:\n",
    "\n",
    "- **An√°lisis de m√©tricas**: Comenten sobre las m√©tricas obtenidas en cada etapa del modelo, destacando las m√°s relevantes como precisi√≥n, recall, F1-score, etc. ¬øCu√°les fueron los modelos m√°s efectivos? ¬øQu√© diferencias notables encontr√≥ entre ellos?\n",
    "\n",
    "- **Impacto de las decisiones tomadas**: Reflexionen sobre c√≥mo las decisiones relacionadas con el preprocesamiento, selecci√≥n de atributos y optimizaci√≥n de hiperpar√°metros influyeron en los resultados finales. ¬øHubo alguna decisi√≥n que haya tenido un impacto notable en el rendimiento?\n",
    "\n",
    "- **Lecciones aprendidas**: Concluyan sobre las lecciones m√°s importantes que aprendieron durante el proceso y c√≥mo estas pueden influir en futuras iteraciones del modelo. ¬øQu√© se podr√≠a mejorar si se repitiera el proceso? Si tuvieran m√°s recursos y tiempo, ¬øqu√© otras t√©cnicas/herramientas habr√≠an utilizado?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [Escriba aqu√≠ sus resultados]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mucho √©xito!\n",
    "\n",
    "<center>\n",
    "<img src=\"https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExaHpvOTY5Z3hpdHI3aDBpdGRueXRqamZncXp2emFrbjJ5M2s5eTR1dSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/1PMVNNKVIL8Ig/giphy.gif\" width=\"300\" height=\"200\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
